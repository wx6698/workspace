{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de544077-954e-4351-9873-0d50ca72930b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47275"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table1 = spark.read.table(\"main.default.query1\")\n",
    "table2 = spark.read.table(\"main.default.query2\")\n",
    "\n",
    "\n",
    "# Compare the tables and find the rows that are in table1 but not in table2\n",
    "d1 = table1.join(table2, on=[\"runId_count\"], how=\"left_anti\")\n",
    "display(d1.count())\n",
    "#display(d1)\n",
    "# Compare the tables and find the rows that are in table2 but not in table1\n",
    "d2 = table2.join(table1, on=[\"runId_count\"], how=\"left_anti\")\n",
    "#display(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa171f75-0316-4996-a9ed-f6c10f6869e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">{\n",
       " &quot;cells&quot;: [\n",
       "  {\n",
       "   &quot;cell_type&quot;: &quot;code&quot;,\n",
       "   &quot;execution_count&quot;: 0,\n",
       "   &quot;metadata&quot;: {\n",
       "    &quot;application/vnd.databricks.v1+cell&quot;: {\n",
       "     &quot;cellMetadata&quot;: {\n",
       "      &quot;byteLimit&quot;: 2048000,\n",
       "      &quot;rowLimit&quot;: 10000\n",
       "     },\n",
       "     &quot;inputWidgets&quot;: {},\n",
       "     &quot;nuid&quot;: &quot;17dd7778-6ab2-4b2e-a360-2c98145e446f&quot;,\n",
       "     &quot;showTitle&quot;: false,\n",
       "     &quot;title&quot;: &quot;&quot;\n",
       "    }\n",
       "   },\n",
       "   &quot;outputs&quot;: [\n",
       "    {\n",
       "     &quot;output_type&quot;: &quot;stream&quot;,\n",
       "     &quot;name&quot;: &quot;stderr&quot;,\n",
       "     &quot;output_type&quot;: &quot;stream&quot;,\n",
       "     &quot;text&quot;: [\n",
       "      &quot;/databricks/python/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 54620 (\\\\N{HANGUL SYLLABLE HAN}) missing from current font.\\n  fig.canvas.print_figure(bytes_io, **kw)\\n/databricks/python/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 44544 (\\\\N{HANGUL SYLLABLE GEUL}) missing from current font.\\n  fig.canvas.print_figure(bytes_io, **kw)\\n/databricks/python/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 54256 (\\\\N{HANGUL SYLLABLE PON}) missing from current font.\\n  fig.canvas.print_figure(bytes_io, **kw)\\n/databricks/python/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 53944 (\\\\N{HANGUL SYLLABLE TEU}) missing from current font.\\n  fig.canvas.print_figure(bytes_io, **kw)\\n/databricks/python/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 50504 (\\\\N{HANGUL SYLLABLE AN}) missing from current font.\\n  fig.canvas.print_figure(bytes_io, **kw)\\n/databricks/python/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 48372 (\\\\N{HANGUL SYLLABLE BO}) missing from current font.\\n  fig.canvas.print_figure(bytes_io, **kw)\\n/databricks/python/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 51060 (\\\\N{HANGUL SYLLABLE I}) missing from current font.\\n  fig.canvas.print_figure(bytes_io, **kw)\\n/databricks/python/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 51648 (\\\\N{HANGUL SYLLABLE JI}) missing from current font.\\n  fig.canvas.print_figure(bytes_io, **kw)\\n/databricks/python/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 47217 (\\\\N{HANGUL SYLLABLE RONG}) missing from current font.\\n  fig.canvas.print_figure(bytes_io, **kw)\\n&quot;\n",
       "     ]\n",
       "    },\n",
       "    {\n",
       "     &quot;output_type&quot;: &quot;display_data&quot;,\n",
       "     &quot;data&quot;: {\n",
       "      &quot;image/png&quot;: &quot;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\\n&quot;,\n",
       "      &quot;text/plain&quot;: [\n",
       "       &quot;&lt;Figure size 640x480 with 1 Axes&gt;&quot;\n",
       "      ]\n",
       "     },\n",
       "     &quot;metadata&quot;: {},\n",
       "     &quot;output_type&quot;: &quot;display_data&quot;\n",
       "    }\n",
       "   ],\n",
       "   &quot;source&quot;: [\n",
       "    &quot;import matplotlib.pyplot as plt\\n&quot;,\n",
       "    &quot;\\n&quot;,\n",
       "    &quot;plt.title('哈哈哈')\\n&quot;,\n",
       "    &quot;plt.show()&quot;\n",
       "   ]\n",
       "  },\n",
       "  {\n",
       "   &quot;cell_type&quot;: &quot;code&quot;,\n",
       "   &quot;execution_count&quot;: 0,\n",
       "   &quot;metadata&quot;: {\n",
       "    &quot;application/vnd.databricks.v1+cell&quot;: {\n",
       "     &quot;cellMetadata&quot;: {},\n",
       "     &quot;inputWidgets&quot;: {},\n",
       "     &quot;nuid&quot;: &quot;d939c4d3-8357-4e44-b9e3-223fdf1fe3aa&quot;,\n",
       "     &quot;showTitle&quot;: false,\n",
       "     &quot;title&quot;: &quot;&quot;\n",
       "    }\n",
       "   },\n",
       "   &quot;outputs&quot;: [],\n",
       "   &quot;source&quot;: []\n",
       "  }\n",
       " ],\n",
       " &quot;metadata&quot;: {\n",
       "  &quot;application/vnd.databricks.v1+notebook&quot;: {\n",
       "   &quot;dashboards&quot;: [],\n",
       "   &quot;language&quot;: &quot;python&quot;,\n",
       "   &quot;notebookMetadata&quot;: {\n",
       "    &quot;pythonIndentUnit&quot;: 4\n",
       "   },\n",
       "   &quot;notebookName&quot;: &quot;matplotlib&quot;,\n",
       "   &quot;widgets&quot;: {}\n",
       "  }\n",
       " },\n",
       " &quot;nbformat&quot;: 4,\n",
       " &quot;nbformat_minor&quot;: 0\n",
       "}\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">{\n &quot;cells&quot;: [\n  {\n   &quot;cell_type&quot;: &quot;code&quot;,\n   &quot;execution_count&quot;: 0,\n   &quot;metadata&quot;: {\n    &quot;application/vnd.databricks.v1+cell&quot;: {\n     &quot;cellMetadata&quot;: {\n      &quot;byteLimit&quot;: 2048000,\n      &quot;rowLimit&quot;: 10000\n     },\n     &quot;inputWidgets&quot;: {},\n     &quot;nuid&quot;: &quot;17dd7778-6ab2-4b2e-a360-2c98145e446f&quot;,\n     &quot;showTitle&quot;: false,\n     &quot;title&quot;: &quot;&quot;\n    }\n   },\n   &quot;outputs&quot;: [\n    {\n     &quot;output_type&quot;: &quot;stream&quot;,\n     &quot;name&quot;: &quot;stderr&quot;,\n     &quot;output_type&quot;: &quot;stream&quot;,\n     &quot;text&quot;: [\n      &quot;/databricks/python/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 54620 (\\\\N{HANGUL SYLLABLE HAN}) missing from current font.\\n  fig.canvas.print_figure(bytes_io, **kw)\\n/databricks/python/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 44544 (\\\\N{HANGUL SYLLABLE GEUL}) missing from current font.\\n  fig.canvas.print_figure(bytes_io, **kw)\\n/databricks/python/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 54256 (\\\\N{HANGUL SYLLABLE PON}) missing from current font.\\n  fig.canvas.print_figure(bytes_io, **kw)\\n/databricks/python/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 53944 (\\\\N{HANGUL SYLLABLE TEU}) missing from current font.\\n  fig.canvas.print_figure(bytes_io, **kw)\\n/databricks/python/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 50504 (\\\\N{HANGUL SYLLABLE AN}) missing from current font.\\n  fig.canvas.print_figure(bytes_io, **kw)\\n/databricks/python/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 48372 (\\\\N{HANGUL SYLLABLE BO}) missing from current font.\\n  fig.canvas.print_figure(bytes_io, **kw)\\n/databricks/python/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 51060 (\\\\N{HANGUL SYLLABLE I}) missing from current font.\\n  fig.canvas.print_figure(bytes_io, **kw)\\n/databricks/python/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 51648 (\\\\N{HANGUL SYLLABLE JI}) missing from current font.\\n  fig.canvas.print_figure(bytes_io, **kw)\\n/databricks/python/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 47217 (\\\\N{HANGUL SYLLABLE RONG}) missing from current font.\\n  fig.canvas.print_figure(bytes_io, **kw)\\n&quot;\n     ]\n    },\n    {\n     &quot;output_type&quot;: &quot;display_data&quot;,\n     &quot;data&quot;: {\n      &quot;image/png&quot;: &quot;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\\n&quot;,\n      &quot;text/plain&quot;: [\n       &quot;&lt;Figure size 640x480 with 1 Axes&gt;&quot;\n      ]\n     },\n     &quot;metadata&quot;: {},\n     &quot;output_type&quot;: &quot;display_data&quot;\n    }\n   ],\n   &quot;source&quot;: [\n    &quot;import matplotlib.pyplot as plt\\n&quot;,\n    &quot;\\n&quot;,\n    &quot;plt.title('哈哈哈')\\n&quot;,\n    &quot;plt.show()&quot;\n   ]\n  },\n  {\n   &quot;cell_type&quot;: &quot;code&quot;,\n   &quot;execution_count&quot;: 0,\n   &quot;metadata&quot;: {\n    &quot;application/vnd.databricks.v1+cell&quot;: {\n     &quot;cellMetadata&quot;: {},\n     &quot;inputWidgets&quot;: {},\n     &quot;nuid&quot;: &quot;d939c4d3-8357-4e44-b9e3-223fdf1fe3aa&quot;,\n     &quot;showTitle&quot;: false,\n     &quot;title&quot;: &quot;&quot;\n    }\n   },\n   &quot;outputs&quot;: [],\n   &quot;source&quot;: []\n  }\n ],\n &quot;metadata&quot;: {\n  &quot;application/vnd.databricks.v1+notebook&quot;: {\n   &quot;dashboards&quot;: [],\n   &quot;language&quot;: &quot;python&quot;,\n   &quot;notebookMetadata&quot;: {\n    &quot;pythonIndentUnit&quot;: 4\n   },\n   &quot;notebookName&quot;: &quot;matplotlib&quot;,\n   &quot;widgets&quot;: {}\n  }\n },\n &quot;nbformat&quot;: 4,\n &quot;nbformat_minor&quot;: 0\n}\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs head dbfs:/xin/matplotlib.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "675e78bc-d55c-4bab-a7c0-84c93b5c26e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47275"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table1.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f1c5cd3-8a63-4b5a-87d7-5adab32517d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120838"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de006d2e-7ed8-4674-84df-54ba508d345e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120838"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2` .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab4ee088-af12-435e-8a48-8117da395b4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[runId_count: bigint]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table1 = spark.read.table(\"main.default.query1\")\n",
    "table1.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b708043-197c-47ec-a2d9-72ede027985a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">import org.apache.spark._\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.catalyst._\n",
       "import org.apache.spark.sql.catalyst.analysis._\n",
       "mcsc: com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog = com.databricks.sql.DatabricksSessionCatalog@58084649\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">import org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.catalyst._\nimport org.apache.spark.sql.catalyst.analysis._\nmcsc: com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog = com.databricks.sql.DatabricksSessionCatalog@58084649\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.catalyst._\n",
    "import org.apache.spark.sql.catalyst.analysis._ \n",
    "\n",
    "val mcsc = spark.sessionState.managedCatalogSessionCatalog\n",
    "\n",
    "for(cat <- mcsc.listCatalogs()) {\n",
    "  if(cat != \"main\" & cat != \"system\" & cat != \"samples\" & cat != \"hive_metastore\") {\n",
    "    for(db <- mcsc.listDatabasesWithCatalog(cat, \"*\", false)) \n",
    "      if (db != \"information_schema\") {\n",
    "        for(i <- mcsc.listCatalogTables(cat, db, \"*\", false)) {\n",
    "          var qname = i.qualifiedName\n",
    "          var qc_name = qname.split('.')(0)\n",
    "          var qd_name = qname.split('.')(1)\n",
    "          var qt_name = qname.split('.')(2)\n",
    "            val tableId = TableIdentifier(qt_name, Some(qd_name), Some(qc_name))\n",
    "            val table = mcsc.getTableMetadata(tableId)\n",
    "\n",
    "            try {\n",
    "                println(\"Catalog: \" + qc_name + \" | Database: \" + qd_name + \" | Table: \" + qt_name + \" | Location: \" +  table.location)\n",
    "            } catch {\n",
    "              case e @ (_ :  org.apache.spark.sql.catalyst.analysis.NoSuchTableException | _ : org.apache.spark.sql.AnalysisException) => println(\"No Location URI..\")\n",
    "            }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a221bfe0-cc1c-431f-b765-032726f0fc6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 68 bytes.\n",
      "Out[4]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.put(\"/tmp/cluster_name.sh\",\"\"\"\n",
    "# /bin/bash\n",
    "touch /tmp/cluster\n",
    "echo $DB_CLUSTER_NAME > /tmp/cluster\"\"\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b072466-35bd-492c-8423-48fa75145bb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Connection to www.google.com (142.250.217.100) 443 port [tcp/https] succeeded!\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "nc -zv www.google.com 443"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fab47a8-ba54-41d2-b946-701d9095317a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>namespace</th><th>viewName</th><th>isTemporary</th><th>isMaterialized</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "namespace",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "viewName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isTemporary",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "isMaterialized",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "--create temporary view xin as select * from main.default.test_table;\n",
    "--select * from xin\n",
    "\n",
    "show views like 'ti%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "297acb75-ba1c-495c-a04a-3accde8e3dc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-1266906618351591>, line 1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m exists \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcatalog\u001b[38;5;241m.\u001b[39mtableExists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev_landing_zone.crs__energydb.ar_ac_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(exists):\n",
       "\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_fqn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m exists.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/catalog.py:216\u001b[0m, in \u001b[0;36mCatalog.tableExists\u001b[0;34m(self, tableName, dbName)\u001b[0m\n",
       "\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtableExists\u001b[39m(\u001b[38;5;28mself\u001b[39m, tableName: \u001b[38;5;28mstr\u001b[39m, dbName: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
       "\u001b[0;32m--> 216\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTableExists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtableName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdbName\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m pdf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pdf\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/catalog.py:49\u001b[0m, in \u001b[0;36mCatalog._execute_and_fetch\u001b[0;34m(self, catalog)\u001b[0m\n",
       "\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_execute_and_fetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, catalog: plan\u001b[38;5;241m.\u001b[39mLogicalPlan) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n",
       "\u001b[0;32m---> 49\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithPlan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatalog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sparkSession\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m pdf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pdf\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/dataframe.py:1655\u001b[0m, in \u001b[0;36mDataFrame.toPandas\u001b[0;34m(self)\u001b[0m\n",
       "\u001b[1;32m   1653\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot collect on empty session.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[1;32m   1654\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plan\u001b[38;5;241m.\u001b[39mto_proto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient)\n",
       "\u001b[0;32m-> 1655\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:865\u001b[0m, in \u001b[0;36mSparkConnectClient.to_pandas\u001b[0;34m(self, plan)\u001b[0m\n",
       "\u001b[1;32m    861\u001b[0m (self_destruct_conf,) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_config_with_defaults(\n",
       "\u001b[1;32m    862\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.execution.arrow.pyspark.selfDestruct.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n",
       "\u001b[1;32m    863\u001b[0m )\n",
       "\u001b[1;32m    864\u001b[0m self_destruct \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, self_destruct_conf)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
       "\u001b[0;32m--> 865\u001b[0m table, schema, metrics, observed_metrics, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\n",
       "\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_destruct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_destruct\u001b[49m\n",
       "\u001b[1;32m    867\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m table \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m    870\u001b[0m schema \u001b[38;5;241m=\u001b[39m schema \u001b[38;5;129;01mor\u001b[39;00m from_arrow_schema(table\u001b[38;5;241m.\u001b[39mschema, prefer_timestamp_ntz\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1284\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch\u001b[0;34m(self, req, self_destruct)\u001b[0m\n",
       "\u001b[1;32m   1281\u001b[0m schema: Optional[StructType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m   1282\u001b[0m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n",
       "\u001b[0;32m-> 1284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch_as_iterator(req):\n",
       "\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, StructType):\n",
       "\u001b[1;32m   1286\u001b[0m         schema \u001b[38;5;241m=\u001b[39m response\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1265\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[0;34m(self, req)\u001b[0m\n",
       "\u001b[1;32m   1263\u001b[0m                     \u001b[38;5;28;01myield from\u001b[39;00m handle_response(b)\n",
       "\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n",
       "\u001b[0;32m-> 1265\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1512\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[0;34m(self, error)\u001b[0m\n",
       "\u001b[1;32m   1499\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
       "\u001b[1;32m   1500\u001b[0m \u001b[38;5;124;03mHandle errors that occur during RPC calls.\u001b[39;00m\n",
       "\u001b[1;32m   1501\u001b[0m \n",
       "\u001b[0;32m   (...)\u001b[0m\n",
       "\u001b[1;32m   1509\u001b[0m \u001b[38;5;124;03mThrows the appropriate internal Python exception.\u001b[39;00m\n",
       "\u001b[1;32m   1510\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
       "\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc\u001b[38;5;241m.\u001b[39mRpcError):\n",
       "\u001b[0;32m-> 1512\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1513\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
       "\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke RPC\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1548\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_rpc_error\u001b[0;34m(self, rpc_error)\u001b[0m\n",
       "\u001b[1;32m   1546\u001b[0m             info \u001b[38;5;241m=\u001b[39m error_details_pb2\u001b[38;5;241m.\u001b[39mErrorInfo()\n",
       "\u001b[1;32m   1547\u001b[0m             d\u001b[38;5;241m.\u001b[39mUnpack(info)\n",
       "\u001b[0;32m-> 1548\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(info, status\u001b[38;5;241m.\u001b[39mmessage) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
       "\u001b[1;32m   1550\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status\u001b[38;5;241m.\u001b[39mmessage) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
       "\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: [UC_COMMAND_NOT_SUPPORTED.WITHOUT_RECOMMENDATION] The command(s): ResolvedIdentifier are not supported in Unity Catalog. ;\n",
       "ResolvedIdentifier com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@491eb527, crs__energydb.ar_ac_manager\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\nFile \u001b[0;32m<command-1266906618351591>, line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m exists \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcatalog\u001b[38;5;241m.\u001b[39mtableExists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev_landing_zone.crs__energydb.ar_ac_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(exists):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_fqn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m exists.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/catalog.py:216\u001b[0m, in \u001b[0;36mCatalog.tableExists\u001b[0;34m(self, tableName, dbName)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtableExists\u001b[39m(\u001b[38;5;28mself\u001b[39m, tableName: \u001b[38;5;28mstr\u001b[39m, dbName: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 216\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTableExists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtableName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdbName\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m pdf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pdf\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/catalog.py:49\u001b[0m, in \u001b[0;36mCatalog._execute_and_fetch\u001b[0;34m(self, catalog)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_execute_and_fetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, catalog: plan\u001b[38;5;241m.\u001b[39mLogicalPlan) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m---> 49\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithPlan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatalog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sparkSession\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m pdf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pdf\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/dataframe.py:1655\u001b[0m, in \u001b[0;36mDataFrame.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot collect on empty session.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1654\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plan\u001b[38;5;241m.\u001b[39mto_proto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient)\n\u001b[0;32m-> 1655\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:865\u001b[0m, in \u001b[0;36mSparkConnectClient.to_pandas\u001b[0;34m(self, plan)\u001b[0m\n\u001b[1;32m    861\u001b[0m (self_destruct_conf,) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_config_with_defaults(\n\u001b[1;32m    862\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.execution.arrow.pyspark.selfDestruct.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    863\u001b[0m )\n\u001b[1;32m    864\u001b[0m self_destruct \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, self_destruct_conf)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 865\u001b[0m table, schema, metrics, observed_metrics, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_destruct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_destruct\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m table \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    870\u001b[0m schema \u001b[38;5;241m=\u001b[39m schema \u001b[38;5;129;01mor\u001b[39;00m from_arrow_schema(table\u001b[38;5;241m.\u001b[39mschema, prefer_timestamp_ntz\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1284\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch\u001b[0;34m(self, req, self_destruct)\u001b[0m\n\u001b[1;32m   1281\u001b[0m schema: Optional[StructType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch_as_iterator(req):\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, StructType):\n\u001b[1;32m   1286\u001b[0m         schema \u001b[38;5;241m=\u001b[39m response\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1265\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1263\u001b[0m                     \u001b[38;5;28;01myield from\u001b[39;00m handle_response(b)\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m-> 1265\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1512\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;124;03mHandle errors that occur during RPC calls.\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;124;03mThrows the appropriate internal Python exception.\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc\u001b[38;5;241m.\u001b[39mRpcError):\n\u001b[0;32m-> 1512\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke RPC\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1548\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_rpc_error\u001b[0;34m(self, rpc_error)\u001b[0m\n\u001b[1;32m   1546\u001b[0m             info \u001b[38;5;241m=\u001b[39m error_details_pb2\u001b[38;5;241m.\u001b[39mErrorInfo()\n\u001b[1;32m   1547\u001b[0m             d\u001b[38;5;241m.\u001b[39mUnpack(info)\n\u001b[0;32m-> 1548\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(info, status\u001b[38;5;241m.\u001b[39mmessage) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1550\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status\u001b[38;5;241m.\u001b[39mmessage) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\n\u001b[0;31mAnalysisException\u001b[0m: [UC_COMMAND_NOT_SUPPORTED.WITHOUT_RECOMMENDATION] The command(s): ResolvedIdentifier are not supported in Unity Catalog. ;\nResolvedIdentifier com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@491eb527, crs__energydb.ar_ac_manager\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UC_COMMAND_NOT_SUPPORTED.WITHOUT_RECOMMENDATION] The command(s): ResolvedIdentifier are not supported in Unity Catalog. ;\nResolvedIdentifier com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@491eb527, crs__energydb.ar_ac_manager\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "exists = spark.catalog.tableExists(\"dev_landing_zone.crs__energydb.ar_ac_manager\")\n",
    "if(exists):\n",
    "    print(f\"{table_fqn} exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8001a6ab-aad6-4120-8a97-d866de631b14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def do_something(a, b):\n",
    "  import pdb; pdb.set_trace()\n",
    "  return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30b5b439-a138-4f60-918b-a2d34083e256",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<command-3136973838622688>\u001b[0m(31)\u001b[0;36mdo_something\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     27 \u001b[0;31m  \u001b[0;31m###################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     28 \u001b[0;31m  \u001b[0;31m###################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     29 \u001b[0;31m  \u001b[0;31m# large function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     30 \u001b[0;31m  \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 31 \u001b[0;31m  \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ipdb>  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "do_something(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a74b7a80-2998-466f-b561-4e7177fc6cb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 216 bytes.\n",
      "Out[2]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.put(\"/tmp/disable_HMS.sh\",\"\"\"\n",
    "# /bin/bash\n",
    "cat > /databricks/common/conf/disable-metastore-monitor.conf << EOL\n",
    "  {\n",
    "   databricks.daemon.driver.enableMetastoreMonitor = false,\n",
    "   databricks.daemon.driver.enableMetastoreHealthCheck = false,\n",
    "  }\n",
    "EOL\"\"\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc67bf9-629e-4e51-9745-758ac26b5278",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n",
      "    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n",
      "    inet 127.0.0.1/8 scope host lo\n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 ::1/128 scope host \n",
      "       valid_lft forever preferred_lft forever\n",
      "5: eth0@if6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc noqueue state UP group default qlen 1000\n",
      "    link/ether 00:16:3e:c7:15:94 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n",
      "    inet 10.101.208.110/16 brd 10.101.255.255 scope global eth0\n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 fe80::216:3eff:fec7:1594/64 scope link \n",
      "       valid_lft forever preferred_lft forever\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "ip addr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c93036a-8aec-4a61-9b34-d3e48fda63a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "is not ready or in a bad state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85196e00-c501-43d1-b9f6-92f39d1cacca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "java.lang.Exception: Unable to start python kernel for ReplId-ad875-b1ba3-42ffe, kernel did not start within 80 seconds.\n",
       "----- stdout -----\n",
       "NOTE: When using the `ipython kernel` entry point, Ctrl-C will not work.\n",
       "\n",
       "To exit, you will have to explicitly quit this process, by either sending\n",
       "&quot;quit&quot; from a client, or using Ctrl-\\ in UNIX-like environments.\n",
       "\n",
       "To read more about this, see https://github.com/ipython/ipython/issues/2049\n",
       "\n",
       "\n",
       "To connect another client to this kernel, use:\n",
       "    --existing /databricks/kernel-connections/8019650cc584e77976fa6b3128925e731691e627c2541fd5886cedc9f435181f.json\n",
       "\n",
       "------------------\n",
       "----- stderr -----\n",
       "Mon Jul  3 23:31:31 2023 Connection to spark from PID  1385\n",
       "Mon Jul  3 23:31:31 2023 Initialized gateway on port 34877\n",
       "Mon Jul  3 23:31:31 2023 Connected to spark.\n",
       "\n",
       "------------------\n",
       "\n",
       "\tat com.databricks.backend.daemon.driver.IpykernelUtils$.startReplFailure$1(JupyterDriverLocal.scala:1037)\n",
       "\tat com.databricks.backend.daemon.driver.IpykernelUtils$.startIpyKernel(JupyterDriverLocal.scala:1062)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.$anonfun$startPython$1(JupyterDriverLocal.scala:783)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.com$databricks$backend$daemon$driver$JupyterDriverLocal$$withRetry(JupyterDriverLocal.scala:727)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal$$anonfun$com$databricks$backend$daemon$driver$JupyterDriverLocal$$withRetry$1.applyOrElse(JupyterDriverLocal.scala:730)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal$$anonfun$com$databricks$backend$daemon$driver$JupyterDriverLocal$$withRetry$1.applyOrElse(JupyterDriverLocal.scala:727)\n",
       "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
       "\tat scala.util.Failure.recover(Try.scala:234)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.com$databricks$backend$daemon$driver$JupyterDriverLocal$$withRetry(JupyterDriverLocal.scala:727)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.startPython(JupyterDriverLocal.scala:765)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.&lt;init&gt;(JupyterDriverLocal.scala:400)\n",
       "\tat com.databricks.backend.daemon.driver.PythonDriverWrapper.instantiateDriver(DriverWrapper.scala:781)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.setupRepl(DriverWrapper.scala:350)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:246)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "java.lang.Exception: Unable to start python kernel for ReplId-ad875-b1ba3-42ffe, kernel did not start within 80 seconds.\n----- stdout -----\nNOTE: When using the `ipython kernel` entry point, Ctrl-C will not work.\n\nTo exit, you will have to explicitly quit this process, by either sending\n&quot;quit&quot; from a client, or using Ctrl-\\ in UNIX-like environments.\n\nTo read more about this, see https://github.com/ipython/ipython/issues/2049\n\n\nTo connect another client to this kernel, use:\n    --existing /databricks/kernel-connections/8019650cc584e77976fa6b3128925e731691e627c2541fd5886cedc9f435181f.json\n\n------------------\n----- stderr -----\nMon Jul  3 23:31:31 2023 Connection to spark from PID  1385\nMon Jul  3 23:31:31 2023 Initialized gateway on port 34877\nMon Jul  3 23:31:31 2023 Connected to spark.\n\n------------------\n\n\tat com.databricks.backend.daemon.driver.IpykernelUtils$.startReplFailure$1(JupyterDriverLocal.scala:1037)\n\tat com.databricks.backend.daemon.driver.IpykernelUtils$.startIpyKernel(JupyterDriverLocal.scala:1062)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.$anonfun$startPython$1(JupyterDriverLocal.scala:783)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.com$databricks$backend$daemon$driver$JupyterDriverLocal$$withRetry(JupyterDriverLocal.scala:727)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal$$anonfun$com$databricks$backend$daemon$driver$JupyterDriverLocal$$withRetry$1.applyOrElse(JupyterDriverLocal.scala:730)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal$$anonfun$com$databricks$backend$daemon$driver$JupyterDriverLocal$$withRetry$1.applyOrElse(JupyterDriverLocal.scala:727)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat scala.util.Failure.recover(Try.scala:234)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.com$databricks$backend$daemon$driver$JupyterDriverLocal$$withRetry(JupyterDriverLocal.scala:727)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.startPython(JupyterDriverLocal.scala:765)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.&lt;init&gt;(JupyterDriverLocal.scala:400)\n\tat com.databricks.backend.daemon.driver.PythonDriverWrapper.instantiateDriver(DriverWrapper.scala:781)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.setupRepl(DriverWrapper.scala:350)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:246)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "Failure starting repl. Try detaching and re-attaching the notebook.\n\n",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh\n",
    "curl http://localhost:40001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cae66a8c-9324-406a-9e17-e02abc14c0dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "java.lang.Exception: Unable to start python kernel for ReplId-6dd96-031c8-0f74d-3, kernel did not start within 80 seconds.\n",
       "----- stdout -----\n",
       "NOTE: When using the `ipython kernel` entry point, Ctrl-C will not work.\n",
       "\n",
       "To exit, you will have to explicitly quit this process, by either sending\n",
       "&quot;quit&quot; from a client, or using Ctrl-\\ in UNIX-like environments.\n",
       "\n",
       "To read more about this, see https://github.com/ipython/ipython/issues/2049\n",
       "\n",
       "\n",
       "To connect another client to this kernel, use:\n",
       "    --existing /databricks/kernel-connections/ce9514a4eb1e58cad733bdcd3ec02ae7f4f6a57c41eb184c01f3ddbf4fa278b5.json\n",
       "\n",
       "------------------\n",
       "----- stderr -----\n",
       "Wed Jun 28 01:38:44 2023 Connection to spark from PID  1554\n",
       "Wed Jun 28 01:38:44 2023 Initialized gateway on port 41571\n",
       "Wed Jun 28 01:38:44 2023 Connected to spark.\n",
       "\n",
       "------------------\n",
       "\n",
       "\tat com.databricks.backend.daemon.driver.IpykernelUtils$.startReplFailure$1(JupyterDriverLocal.scala:1037)\n",
       "\tat com.databricks.backend.daemon.driver.IpykernelUtils$.startIpyKernel(JupyterDriverLocal.scala:1062)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.$anonfun$startPython$1(JupyterDriverLocal.scala:783)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.com$databricks$backend$daemon$driver$JupyterDriverLocal$$withRetry(JupyterDriverLocal.scala:727)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal$$anonfun$com$databricks$backend$daemon$driver$JupyterDriverLocal$$withRetry$1.applyOrElse(JupyterDriverLocal.scala:730)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal$$anonfun$com$databricks$backend$daemon$driver$JupyterDriverLocal$$withRetry$1.applyOrElse(JupyterDriverLocal.scala:727)\n",
       "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
       "\tat scala.util.Failure.recover(Try.scala:234)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.com$databricks$backend$daemon$driver$JupyterDriverLocal$$withRetry(JupyterDriverLocal.scala:727)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.startPython(JupyterDriverLocal.scala:765)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.&lt;init&gt;(JupyterDriverLocal.scala:400)\n",
       "\tat com.databricks.backend.daemon.driver.PythonDriverWrapper.instantiateDriver(DriverWrapper.scala:781)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.setupRepl(DriverWrapper.scala:350)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:246)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "java.lang.Exception: Unable to start python kernel for ReplId-6dd96-031c8-0f74d-3, kernel did not start within 80 seconds.\n----- stdout -----\nNOTE: When using the `ipython kernel` entry point, Ctrl-C will not work.\n\nTo exit, you will have to explicitly quit this process, by either sending\n&quot;quit&quot; from a client, or using Ctrl-\\ in UNIX-like environments.\n\nTo read more about this, see https://github.com/ipython/ipython/issues/2049\n\n\nTo connect another client to this kernel, use:\n    --existing /databricks/kernel-connections/ce9514a4eb1e58cad733bdcd3ec02ae7f4f6a57c41eb184c01f3ddbf4fa278b5.json\n\n------------------\n----- stderr -----\nWed Jun 28 01:38:44 2023 Connection to spark from PID  1554\nWed Jun 28 01:38:44 2023 Initialized gateway on port 41571\nWed Jun 28 01:38:44 2023 Connected to spark.\n\n------------------\n\n\tat com.databricks.backend.daemon.driver.IpykernelUtils$.startReplFailure$1(JupyterDriverLocal.scala:1037)\n\tat com.databricks.backend.daemon.driver.IpykernelUtils$.startIpyKernel(JupyterDriverLocal.scala:1062)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.$anonfun$startPython$1(JupyterDriverLocal.scala:783)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.com$databricks$backend$daemon$driver$JupyterDriverLocal$$withRetry(JupyterDriverLocal.scala:727)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal$$anonfun$com$databricks$backend$daemon$driver$JupyterDriverLocal$$withRetry$1.applyOrElse(JupyterDriverLocal.scala:730)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal$$anonfun$com$databricks$backend$daemon$driver$JupyterDriverLocal$$withRetry$1.applyOrElse(JupyterDriverLocal.scala:727)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat scala.util.Failure.recover(Try.scala:234)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.com$databricks$backend$daemon$driver$JupyterDriverLocal$$withRetry(JupyterDriverLocal.scala:727)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.startPython(JupyterDriverLocal.scala:765)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.&lt;init&gt;(JupyterDriverLocal.scala:400)\n\tat com.databricks.backend.daemon.driver.PythonDriverWrapper.instantiateDriver(DriverWrapper.scala:781)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.setupRepl(DriverWrapper.scala:350)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:246)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "Failure starting repl. Try detaching and re-attaching the notebook.\n\n",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh\n",
    "ps -ef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b860c22-efb1-4083-8a7e-ecfd4794f8b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "nc -l 1521"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b389e04-54f2-4c2a-bbfc-43bfee3dcd7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<configuration>\n",
      "  \n",
      "<property>\n",
      "  <name>javax.jdo.option.ConnectionURL</name>\n",
      "  <value>jdbc:mariadb://mdpartyyphlhsp.caj77bnxuhme.us-west-2.rds.amazonaws.com:3306/organization4458556048801738?useSSL=true&amp;enabledSslProtocolSuites=TLSv1,TLSv1.1,TLSv1.2&amp;serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt</value>\n",
      "  <description>JDBC connect string for a JDBC metastore</description>\n",
      "</property>\n",
      "<property>\n",
      "  <name>javax.jdo.option.ConnectionDriverName</name>\n",
      "  <value>org.mariadb.jdbc.Driver</value>\n",
      "  <description>Driver class name for a JDBC metastore</description>\n",
      "</property>\n",
      "<property>\n",
      "  <name>javax.jdo.option.ConnectionUserName</name>\n",
      "  <value>[REDACTED]</value>\n",
      "  <description>Username to use against metastore database</description>\n",
      "</property>\n",
      "<property>\n",
      "  <name>javax.jdo.option.ConnectionPassword</name>\n",
      "  <value>[REDACTED]</value>\n",
      "  <description>Password to use against metastore database</description>\n",
      "</property>\n",
      "<!-- If the following two properties are not set correctly, the metastore will\n",
      "     attempt to initialize its schema upon startup\n",
      "-->\n",
      "<property>\n",
      "  <name>datanucleus.schema.autoCreateAll</name>\n",
      "  <value>false</value>\n",
      "</property>\n",
      "<property>\n",
      "  <name>datanucleus.autoCreateSchema</name>\n",
      "  <value>false</value>\n",
      "</property>\n",
      "<property>\n",
      "  <name>datanucleus.fixedDatastore</name>\n",
      "    <value>true</value>\n",
      "</property>\n",
      "<property>\n",
      "  <name>datanucleus.connectionPool.minPoolSize</name>\n",
      "    <value>0</value>\n",
      "</property>\n",
      "<property>\n",
      "  <name>datanucleus.connectionPool.initialPoolSize</name>\n",
      "    <value>0</value>\n",
      "</property>\n",
      "<property>\n",
      "  <name>datanucleus.connectionPool.maxPoolSize</name>\n",
      "    <value>1</value>\n",
      "</property>\n",
      "<property>\n",
      "  <name>hive.stats.autogather</name>\n",
      "  <value>false</value>\n",
      "</property>\n",
      "       \n",
      "  <property>\n",
      "    <name>mapred.reduce.tasks</name>\n",
      "    <value>100</value>\n",
      "  </property>\n",
      "  <!-- To mitigate the problem of PROD-4498 and per HIVE-7140, we need to bump the timeout.\n",
      "       Since the default value of this property used by Impala is 3600 seconds, we will use this value for\n",
      "       actual deployment\n",
      "       (http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cm_props_cdh530_impala.html).\n",
      "  -->\n",
      "  <property>\n",
      "    <name>hive.metastore.client.socket.timeout</name>\n",
      "    <value>3600</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>hadoop.tmp.dir</name>\n",
      "    <value>/local_disk0/tmp</value>\n",
      "  </property>\n",
      "\n",
      "  <property>\n",
      "    <name>hive.metastore.client.connect.retry.delay</name>\n",
      "    <value>10</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>hive.metastore.failure.retries</name>\n",
      "    <value>30</value>\n",
      "  </property>\n",
      "\n",
      "\n",
      "</configuration>\n",
      "     "
     ]
    }
   ],
   "source": [
    "%sh\n",
    "cat /databricks/hive/conf/hive-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51df3a15-b81c-4eb1-8610-f4d4205d2332",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nc: connect to mdpartyyphlhsp.caj77bnxuhme.us-west-2.rds.amazonaws.com port 3306 (tcp) failed: Connection refused\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "nc -zv mdpartyyphlhsp.caj77bnxuhme.us-west-2.rds.amazonaws.com 3306"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b5cc59f-130e-4ce3-9f45-974cb421ddf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  7 54.8M    7 4207k    0     0  28.3M      0  0:00:01 --:--:--  0:00:01 28.1M\n",
      "100 54.8M  100 54.8M    0     0  85.6M      0 --:--:-- --:--:-- --:--:-- 85.5M\n",
      "Archive:  awscliv2.zip\n",
      "   creating: aws/\n",
      "   creating: aws/dist/\n",
      "  inflating: aws/THIRD_PARTY_LICENSES  \n",
      "  inflating: aws/install             \n",
      "  inflating: aws/README.md           \n",
      "   creating: aws/dist/awscli/\n",
      "   creating: aws/dist/cryptography/\n",
      "   creating: aws/dist/docutils/\n",
      "   creating: aws/dist/lib-dynload/\n",
      "  inflating: aws/dist/aws            \n",
      "  inflating: aws/dist/aws_completer  \n",
      "  inflating: aws/dist/libpython3.11.so.1.0  \n",
      "  inflating: aws/dist/_awscrt.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/_cffi_backend.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/_ruamel_yaml.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/libz.so.1      \n",
      "  inflating: aws/dist/liblzma.so.0   \n",
      "  inflating: aws/dist/libbz2.so.1    \n",
      "  inflating: aws/dist/libffi.so.5    \n",
      "  inflating: aws/dist/libsqlite3.so.0  \n",
      "  inflating: aws/dist/base_library.zip  \n",
      "  inflating: aws/dist/lib-dynload/_hashlib.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_sha3.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_blake2.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_sha256.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_md5.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_sha1.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_sha512.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_random.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_bisect.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/math.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/binascii.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_contextvars.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_decimal.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_datetime.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/array.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_socket.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/select.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_posixsubprocess.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_typing.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_csv.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/grp.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/resource.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_lzma.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_bz2.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_opcode.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/mmap.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_ctypes.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_queue.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/fcntl.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/termios.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/unicodedata.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_multibytecodec.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_codecs_jp.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_codecs_kr.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_codecs_iso2022.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_codecs_cn.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_codecs_tw.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_codecs_hk.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_heapq.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_json.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_asyncio.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_sqlite3.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_struct.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/zlib.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_posixshmem.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_multiprocessing.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/pyexpat.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_ssl.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_statistics.cpython-311-x86_64-linux-gnu.so  \n",
      "  inflating: aws/dist/lib-dynload/_elementtree.cpython-311-x86_64-linux-gnu.so  \n",
      "   creating: aws/dist/cryptography/hazmat/\n",
      "   creating: aws/dist/cryptography/hazmat/bindings/\n",
      "  inflating: aws/dist/cryptography/hazmat/bindings/_openssl.abi3.so  \n",
      "   creating: aws/dist/awscli/botocore/\n",
      "   creating: aws/dist/awscli/customizations/\n",
      "   creating: aws/dist/awscli/data/\n",
      "   creating: aws/dist/awscli/examples/\n",
      "   creating: aws/dist/awscli/topics/\n",
      "   creating: aws/dist/awscli/examples/acm/\n",
      "   creating: aws/dist/awscli/examples/acm-pca/\n",
      "   creating: aws/dist/awscli/examples/alexaforbusiness/\n",
      "   creating: aws/dist/awscli/examples/apigateway/\n",
      "   creating: aws/dist/awscli/examples/apigatewaymanagementapi/\n",
      "   creating: aws/dist/awscli/examples/apigatewayv2/\n",
      "   creating: aws/dist/awscli/examples/appconfig/\n",
      "   creating: aws/dist/awscli/examples/application-autoscaling/\n",
      "   creating: aws/dist/awscli/examples/appmesh/\n",
      "   creating: aws/dist/awscli/examples/apprunner/\n",
      "   creating: aws/dist/awscli/examples/athena/\n",
      "   creating: aws/dist/awscli/examples/autoscaling/\n",
      "   creating: aws/dist/awscli/examples/autoscaling-plans/\n",
      "   creating: aws/dist/awscli/examples/backup/\n",
      "   creating: aws/dist/awscli/examples/batch/\n",
      "   creating: aws/dist/awscli/examples/budgets/\n",
      "   creating: aws/dist/awscli/examples/ce/\n",
      "   creating: aws/dist/awscli/examples/chime/\n",
      "   creating: aws/dist/awscli/examples/cloud9/\n",
      "   creating: aws/dist/awscli/examples/cloudcontrol/\n",
      "   creating: aws/dist/awscli/examples/cloudformation/\n",
      "   creating: aws/dist/awscli/examples/cloudfront/\n",
      "   creating: aws/dist/awscli/examples/cloudsearchdomain/\n",
      "   creating: aws/dist/awscli/examples/cloudtrail/\n",
      "   creating: aws/dist/awscli/examples/cloudwatch/\n",
      "   creating: aws/dist/awscli/examples/codeartifact/\n",
      "   creating: aws/dist/awscli/examples/codebuild/\n",
      "   creating: aws/dist/awscli/examples/codecommit/\n",
      "   creating: aws/dist/awscli/examples/codeguru-reviewer/\n",
      "   creating: aws/dist/awscli/examples/codepipeline/\n",
      "   creating: aws/dist/awscli/examples/codestar/\n",
      "   creating: aws/dist/awscli/examples/codestar-connections/\n",
      "   creating: aws/dist/awscli/examples/codestar-notifications/\n",
      "   creating: aws/dist/awscli/examples/cognito-identity/\n",
      "   creating: aws/dist/awscli/examples/cognito-idp/\n",
      "   creating: aws/dist/awscli/examples/comprehendmedical/\n",
      "   creating: aws/dist/awscli/examples/configservice/\n",
      "   creating: aws/dist/awscli/examples/configure/\n",
      "   creating: aws/dist/awscli/examples/connect/\n",
      "   creating: aws/dist/awscli/examples/cur/\n",
      "   creating: aws/dist/awscli/examples/datapipeline/\n",
      "   creating: aws/dist/awscli/examples/dax/\n",
      "   creating: aws/dist/awscli/examples/ddb/\n",
      "   creating: aws/dist/awscli/examples/deploy/\n",
      "   creating: aws/dist/awscli/examples/detective/\n",
      "   creating: aws/dist/awscli/examples/devicefarm/\n",
      "   creating: aws/dist/awscli/examples/directconnect/\n",
      "   creating: aws/dist/awscli/examples/discovery/\n",
      "   creating: aws/dist/awscli/examples/dlm/\n",
      "   creating: aws/dist/awscli/examples/dms/\n",
      "   creating: aws/dist/awscli/examples/docdb/\n",
      "   creating: aws/dist/awscli/examples/ds/\n",
      "   creating: aws/dist/awscli/examples/dynamodb/\n",
      "   creating: aws/dist/awscli/examples/dynamodbstreams/\n",
      "   creating: aws/dist/awscli/examples/ec2/\n",
      "   creating: aws/dist/awscli/examples/ec2-instance-connect/\n",
      "   creating: aws/dist/awscli/examples/ecr/\n",
      "   creating: aws/dist/awscli/examples/ecr-public/\n",
      "   creating: aws/dist/awscli/examples/ecs/\n",
      "   creating: aws/dist/awscli/examples/efs/\n",
      "   creating: aws/dist/awscli/examples/eks/\n",
      "   creating: aws/dist/awscli/examples/elasticache/\n",
      "   creating: aws/dist/awscli/examples/elasticbeanstalk/\n",
      "   creating: aws/dist/awscli/examples/elastictranscoder/\n",
      "   creating: aws/dist/awscli/examples/elb/\n",
      "   creating: aws/dist/awscli/examples/elbv2/\n",
      "   creating: aws/dist/awscli/examples/emr/\n",
      "   creating: aws/dist/awscli/examples/emr-containers/\n",
      "   creating: aws/dist/awscli/examples/es/\n",
      "   creating: aws/dist/awscli/examples/events/\n",
      "   creating: aws/dist/awscli/examples/firehose/\n",
      "   creating: aws/dist/awscli/examples/fis/\n",
      "   creating: aws/dist/awscli/examples/fms/\n",
      "   creating: aws/dist/awscli/examples/gamelift/\n",
      "   creating: aws/dist/awscli/examples/glacier/\n",
      "   creating: aws/dist/awscli/examples/globalaccelerator/\n",
      "   creating: aws/dist/awscli/examples/glue/\n",
      "   creating: aws/dist/awscli/examples/grafana/\n",
      "   creating: aws/dist/awscli/examples/greengrass/\n",
      "   creating: aws/dist/awscli/examples/greengrassv2/\n",
      "   creating: aws/dist/awscli/examples/guardduty/\n",
      "   creating: aws/dist/awscli/examples/health/\n",
      "   creating: aws/dist/awscli/examples/healthlake/\n",
      "   creating: aws/dist/awscli/examples/iam/\n",
      "   creating: aws/dist/awscli/examples/imagebuilder/\n",
      "   creating: aws/dist/awscli/examples/importexport/\n",
      "   creating: aws/dist/awscli/examples/inspector/\n",
      "   creating: aws/dist/awscli/examples/iot/\n",
      "   creating: aws/dist/awscli/examples/iot-data/\n",
      "   creating: aws/dist/awscli/examples/iot-jobs-data/\n",
      "   creating: aws/dist/awscli/examples/iot1click-devices/\n",
      "   creating: aws/dist/awscli/examples/iot1click-projects/\n",
      "   creating: aws/dist/awscli/examples/iotanalytics/\n",
      "   creating: aws/dist/awscli/examples/iotdeviceadvisor/\n",
      "   creating: aws/dist/awscli/examples/iotevents/\n",
      "   creating: aws/dist/awscli/examples/iotevents-data/\n",
      "   creating: aws/dist/awscli/examples/iotsitewise/\n",
      "   creating: aws/dist/awscli/examples/iotthingsgraph/\n",
      "   creating: aws/dist/awscli/examples/iotwireless/\n",
      "   creating: aws/dist/awscli/examples/ivs/\n",
      "   creating: aws/dist/awscli/examples/ivs-realtime/\n",
      "   creating: aws/dist/awscli/examples/ivschat/\n",
      "   creating: aws/dist/awscli/examples/kafka/\n",
      "   creating: aws/dist/awscli/examples/kinesis/\n",
      "   creating: aws/dist/awscli/examples/kms/\n",
      "   creating: aws/dist/awscli/examples/lakeformation/\n",
      "   creating: aws/dist/awscli/examples/lambda/\n",
      "   creating: aws/dist/awscli/examples/license-manager/\n",
      "   creating: aws/dist/awscli/examples/lightsail/\n",
      "   creating: aws/dist/awscli/examples/logs/\n",
      "   creating: aws/dist/awscli/examples/macie2/\n",
      "   creating: aws/dist/awscli/examples/mediaconnect/\n",
      "   creating: aws/dist/awscli/examples/mediaconvert/\n",
      "   creating: aws/dist/awscli/examples/medialive/\n",
      "   creating: aws/dist/awscli/examples/mediapackage/\n",
      "   creating: aws/dist/awscli/examples/mediapackage-vod/\n",
      "   creating: aws/dist/awscli/examples/mediastore/\n",
      "   creating: aws/dist/awscli/examples/mediastore-data/\n",
      "   creating: aws/dist/awscli/examples/mediatailor/\n",
      "   creating: aws/dist/awscli/examples/memorydb/\n",
      "   creating: aws/dist/awscli/examples/networkmanager/\n",
      "   creating: aws/dist/awscli/examples/nimble/\n",
      "   creating: aws/dist/awscli/examples/omics/\n",
      "   creating: aws/dist/awscli/examples/opsworks/\n",
      "   creating: aws/dist/awscli/examples/opsworkscm/\n",
      "   creating: aws/dist/awscli/examples/organizations/\n",
      "   creating: aws/dist/awscli/examples/outposts/\n",
      "   creating: aws/dist/awscli/examples/pi/\n",
      "   creating: aws/dist/awscli/examples/pinpoint/\n",
      "   creating: aws/dist/awscli/examples/polly/\n",
      "   creating: aws/dist/awscli/examples/pricing/\n",
      "   creating: aws/dist/awscli/examples/proton/\n",
      "   creating: aws/dist/awscli/examples/qldb/\n",
      "   creating: aws/dist/awscli/examples/ram/\n",
      "   creating: aws/dist/awscli/examples/rds/\n",
      "   creating: aws/dist/awscli/examples/rds-data/\n",
      "   creating: aws/dist/awscli/examples/redshift/\n",
      "   creating: aws/dist/awscli/examples/rekognition/\n",
      "   creating: aws/dist/awscli/examples/resource-explorer-2/\n",
      "   creating: aws/dist/awscli/examples/resource-groups/\n",
      "   creating: aws/dist/awscli/examples/resourcegroupstaggingapi/\n",
      "   creating: aws/dist/awscli/examples/robomaker/\n",
      "   creating: aws/dist/awscli/examples/route53/\n",
      "   creating: aws/dist/awscli/examples/route53domains/\n",
      "   creating: aws/dist/awscli/examples/route53resolver/\n",
      "   creating: aws/dist/awscli/examples/s3/\n",
      "   creating: aws/dist/awscli/examples/s3api/\n",
      "   creating: aws/dist/awscli/examples/s3control/\n",
      "   creating: aws/dist/awscli/examples/secretsmanager/\n",
      "   creating: aws/dist/awscli/examples/securityhub/\n",
      "   creating: aws/dist/awscli/examples/serverlessrepo/\n",
      "   creating: aws/dist/awscli/examples/service-quotas/\n",
      "   creating: aws/dist/awscli/examples/servicecatalog/\n",
      "   creating: aws/dist/awscli/examples/servicecatalog-appregistry/\n",
      "   creating: aws/dist/awscli/examples/servicediscovery/\n",
      "   creating: aws/dist/awscli/examples/ses/\n",
      "   creating: aws/dist/awscli/examples/shield/\n",
      "   creating: aws/dist/awscli/examples/signer/\n",
      "   creating: aws/dist/awscli/examples/snowball/\n",
      "   creating: aws/dist/awscli/examples/sns/\n",
      "   creating: aws/dist/awscli/examples/sqs/\n",
      "   creating: aws/dist/awscli/examples/ssm/\n",
      "   creating: aws/dist/awscli/examples/ssm-contacts/\n",
      "   creating: aws/dist/awscli/examples/ssm-incidents/\n",
      "   creating: aws/dist/awscli/examples/storagegateway/\n",
      "   creating: aws/dist/awscli/examples/sts/\n",
      "   creating: aws/dist/awscli/examples/support/\n",
      "   creating: aws/dist/awscli/examples/swf/\n",
      "   creating: aws/dist/awscli/examples/textract/\n",
      "   creating: aws/dist/awscli/examples/transcribe/\n",
      "   creating: aws/dist/awscli/examples/translate/\n",
      "   creating: aws/dist/awscli/examples/waf/\n",
      "   creating: aws/dist/awscli/examples/waf-regional/\n",
      "   creating: aws/dist/awscli/examples/wafv2/\n",
      "   creating: aws/dist/awscli/examples/workdocs/\n",
      "   creating: aws/dist/awscli/examples/workmail/\n",
      "   creating: aws/dist/awscli/examples/workmailmessageflow/\n",
      "   creating: aws/dist/awscli/examples/workspaces/\n",
      "   creating: aws/dist/awscli/examples/xray/\n",
      "  inflating: aws/dist/awscli/examples/global_options.rst  \n",
      "  inflating: aws/dist/awscli/examples/global_synopsis.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-dimension.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/set-default-authorizer.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/create-domain-configuration.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-custom-metric.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-security-profiles.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/get-effective-policies.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/create-scheduled-audit.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/update-billing-group.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/deprecate-thing-type.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/describe-thing-group.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-audit-mitigation-actions-executions.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/add-thing-to-thing-group.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/update-provisioning-template.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-billing-groups.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-domain-configuration.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/create-certificate-from-csr.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/update-indexing-configuration.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-policies.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/update-dynamic-thing-group.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-mitigations-actions.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/describe-job-execution.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-principal-things.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-scheduled-audits.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/update-stream.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/create-thing-group.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/reject-certificate-transfer.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-thing-groups-for-thing.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-mitigation-actions.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/update-custom-metric.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/describe-event-configurations.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-dynamic-thing-group.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/detach-thing-principal.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/create-authorizer.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/create-topic-rule-destination.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/clear-default-authorizer.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-authorizers.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-certificates.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/describe-stream.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/get-ota-update.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-account-audit-configuration.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/create-thing-type.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-audit-tasks.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/register-thing.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-thing-group.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/set-v2-logging-level.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/create-thing.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/update-thing.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-violation-events.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/enable-topic-rule.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/start-audit-mitigation-actions-task.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/describe-default-authorizer.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-domain-configurations.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/cancel-certificate-transfer.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/describe-dimension.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/create-topic-rule.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/describe-certificate.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/accept-certificate-transfer.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/register-ca-certificate.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-certificate.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-thing-groups.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/get-policy.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-certificates-by-ca.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/update-audit-suppression.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/test-authorization.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-things.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/cancel-audit-mitigation-actions-task.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/attach-thing-principal.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/create-mitigation-action.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-ota-update.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/get-percentiles.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/create-provisioning-claim.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-scheduled-audit.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/update-certificate.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/get-registration-code.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-thing.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-job-executions-for-thing.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-provisioning-template-version.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/cancel-job.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/describe-security-profile.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/create-billing-group.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-things-in-thing-group.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/describe-index.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-topic-rule-destinations.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/get-policy-version.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/describe-audit-task.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/create-dynamic-thing-group.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/update-domain-configuration.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/update-dimension.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/describe-domain-configuration.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/create-policy-version.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/describe-authorizer.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-policy-version.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/describe-audit-finding.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/create-audit-suppression.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-provisioning-template.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-topic-rules.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-stream.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-custom-metrics.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/cancel-audit-task.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/register-certificate.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-authorizer.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-active-violations.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-thing-type.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/update-topic-rule-destination.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/transfer-certificate.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-attached-policies.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-policy-versions.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-billing-group.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/create-ota-update.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-policy.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-security-profiles-for-target.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/describe-ca-certificate.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-role-aliases.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/confirm-topic-rule-destination.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-provisioning-templates.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/update-thing-group.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-outgoing-certificates.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/attach-policy.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/update-security-profile.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/describe-role-alias.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/update-mitigation-action.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-topic-rule-destination.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/set-v2-logging-options.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-targets-for-policy.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-registration-code.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-streams.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/set-default-policy-version.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/create-role-alias.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-indices.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-ca-certificate.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-ca-certificates.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-job-executions-for-job.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-audit-findings.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/update-account-audit-configuration.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/get-behavior-model-training-summaries.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/tag-resource.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/attach-security-profile.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/create-provisioning-template-version.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/update-scheduled-audit.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-v2-logging-levels.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/describe-provisioning-template.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/describe-job.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/delete-mitigation-action.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-dimensions.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/create-job.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/get-statistics.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/create-custom-metric.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/list-things-in-billing-group.rst  \n",
      "  inflating: aws/dist/awscli/examples/iot/describe-thing-\n",
      "\n",
      "*** WARNING: max output size exceeded, skipping output. ***\n",
      "\n",
      "efs/2015-02-01/\n",
      "  inflating: aws/dist/awscli/data/efs/2015-02-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/greengrass/2017-06-07/\n",
      "  inflating: aws/dist/awscli/data/greengrass/2017-06-07/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/kinesisvideo/2017-09-30/\n",
      "  inflating: aws/dist/awscli/data/kinesisvideo/2017-09-30/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/mturk/2017-01-17/\n",
      "  inflating: aws/dist/awscli/data/mturk/2017-01-17/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/iot1click-projects/2018-05-14/\n",
      "  inflating: aws/dist/awscli/data/iot1click-projects/2018-05-14/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/route53domains/2014-05-15/\n",
      "  inflating: aws/dist/awscli/data/route53domains/2014-05-15/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/sdb/2009-04-15/\n",
      "  inflating: aws/dist/awscli/data/sdb/2009-04-15/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/dynamodb/2012-08-10/\n",
      "  inflating: aws/dist/awscli/data/dynamodb/2012-08-10/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/alexaforbusiness/2017-11-09/\n",
      "  inflating: aws/dist/awscli/data/alexaforbusiness/2017-11-09/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/budgets/2016-10-20/\n",
      "  inflating: aws/dist/awscli/data/budgets/2016-10-20/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/secretsmanager/2017-10-17/\n",
      "  inflating: aws/dist/awscli/data/secretsmanager/2017-10-17/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/appstream/2016-12-01/\n",
      "  inflating: aws/dist/awscli/data/appstream/2016-12-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/redshift/2012-12-01/\n",
      "  inflating: aws/dist/awscli/data/redshift/2012-12-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/glacier/2012-06-01/\n",
      "  inflating: aws/dist/awscli/data/glacier/2012-06-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/ec2/2016-11-15/\n",
      "  inflating: aws/dist/awscli/data/ec2/2016-11-15/completions-1.sdk-extras.json  \n",
      "  inflating: aws/dist/awscli/data/ec2/2016-11-15/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/dynamodbstreams/2012-08-10/\n",
      "  inflating: aws/dist/awscli/data/dynamodbstreams/2012-08-10/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/directconnect/2012-10-25/\n",
      "  inflating: aws/dist/awscli/data/directconnect/2012-10-25/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/apigateway/2015-07-09/\n",
      "  inflating: aws/dist/awscli/data/apigateway/2015-07-09/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/events/2015-10-07/\n",
      "  inflating: aws/dist/awscli/data/events/2015-10-07/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/medialive/2017-10-14/\n",
      "  inflating: aws/dist/awscli/data/medialive/2017-10-14/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/codecommit/2015-04-13/\n",
      "  inflating: aws/dist/awscli/data/codecommit/2015-04-13/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/kinesis/2013-12-02/\n",
      "  inflating: aws/dist/awscli/data/kinesis/2013-12-02/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/autoscaling-plans/2018-01-06/\n",
      "  inflating: aws/dist/awscli/data/autoscaling-plans/2018-01-06/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/clouddirectory/2017-01-11/\n",
      "  inflating: aws/dist/awscli/data/clouddirectory/2017-01-11/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/kms/2014-11-01/\n",
      "  inflating: aws/dist/awscli/data/kms/2014-11-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/kinesisanalytics/2015-08-14/\n",
      "  inflating: aws/dist/awscli/data/kinesisanalytics/2015-08-14/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/resourcegroupstaggingapi/2017-01-26/\n",
      "  inflating: aws/dist/awscli/data/resourcegroupstaggingapi/2017-01-26/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/meteringmarketplace/2016-01-14/\n",
      "  inflating: aws/dist/awscli/data/meteringmarketplace/2016-01-14/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/dax/2017-04-19/\n",
      "  inflating: aws/dist/awscli/data/dax/2017-04-19/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/mobile/2017-07-01/\n",
      "  inflating: aws/dist/awscli/data/mobile/2017-07-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/codestar/2017-04-19/\n",
      "  inflating: aws/dist/awscli/data/codestar/2017-04-19/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/sqs/2012-11-05/\n",
      "  inflating: aws/dist/awscli/data/sqs/2012-11-05/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/elasticache/2015-02-02/\n",
      "  inflating: aws/dist/awscli/data/elasticache/2015-02-02/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/iot/2015-05-28/\n",
      "  inflating: aws/dist/awscli/data/iot/2015-05-28/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/cloudfront/2018-06-18/\n",
      "   creating: aws/dist/awscli/data/cloudfront/2019-03-26/\n",
      "   creating: aws/dist/awscli/data/cloudfront/2020-05-31/\n",
      "  inflating: aws/dist/awscli/data/cloudfront/2020-05-31/completions-1.json  \n",
      "  inflating: aws/dist/awscli/data/cloudfront/2019-03-26/completions-1.json  \n",
      "  inflating: aws/dist/awscli/data/cloudfront/2018-06-18/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/pricing/2017-10-15/\n",
      "  inflating: aws/dist/awscli/data/pricing/2017-10-15/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/firehose/2015-08-04/\n",
      "  inflating: aws/dist/awscli/data/firehose/2015-08-04/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/elb/2012-06-01/\n",
      "  inflating: aws/dist/awscli/data/elb/2012-06-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/rds/2014-10-31/\n",
      "  inflating: aws/dist/awscli/data/rds/2014-10-31/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/dlm/2018-01-12/\n",
      "  inflating: aws/dist/awscli/data/dlm/2018-01-12/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/cloudtrail/2013-11-01/\n",
      "  inflating: aws/dist/awscli/data/cloudtrail/2013-11-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/mediatailor/2018-04-23/\n",
      "  inflating: aws/dist/awscli/data/mediatailor/2018-04-23/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/emr/2009-03-31/\n",
      "  inflating: aws/dist/awscli/data/emr/2009-03-31/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/dms/2016-01-01/\n",
      "  inflating: aws/dist/awscli/data/dms/2016-01-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/storagegateway/2013-06-30/\n",
      "  inflating: aws/dist/awscli/data/storagegateway/2013-06-30/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/inspector/2016-02-16/\n",
      "  inflating: aws/dist/awscli/data/inspector/2016-02-16/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/cognito-identity/2014-06-30/\n",
      "  inflating: aws/dist/awscli/data/cognito-identity/2014-06-30/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/route53/2013-04-01/\n",
      "  inflating: aws/dist/awscli/data/route53/2013-04-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/s3/2006-03-01/\n",
      "  inflating: aws/dist/awscli/data/s3/2006-03-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/opsworks/2013-02-18/\n",
      "  inflating: aws/dist/awscli/data/opsworks/2013-02-18/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/health/2016-08-04/\n",
      "  inflating: aws/dist/awscli/data/health/2016-08-04/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/rekognition/2016-06-27/\n",
      "  inflating: aws/dist/awscli/data/rekognition/2016-06-27/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/signer/2017-08-25/\n",
      "  inflating: aws/dist/awscli/data/signer/2017-08-25/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/mediapackage/2017-10-12/\n",
      "  inflating: aws/dist/awscli/data/mediapackage/2017-10-12/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/waf/2015-08-24/\n",
      "  inflating: aws/dist/awscli/data/waf/2015-08-24/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/application-autoscaling/2016-02-06/\n",
      "  inflating: aws/dist/awscli/data/application-autoscaling/2016-02-06/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/codedeploy/2014-10-06/\n",
      "  inflating: aws/dist/awscli/data/codedeploy/2014-10-06/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/mgh/2017-05-31/\n",
      "  inflating: aws/dist/awscli/data/mgh/2017-05-31/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/acm-pca/2017-08-22/\n",
      "  inflating: aws/dist/awscli/data/acm-pca/2017-08-22/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/cloud9/2017-09-23/\n",
      "  inflating: aws/dist/awscli/data/cloud9/2017-09-23/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/transcribe/2017-10-26/\n",
      "  inflating: aws/dist/awscli/data/transcribe/2017-10-26/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/discovery/2015-11-01/\n",
      "  inflating: aws/dist/awscli/data/discovery/2015-11-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/ce/2017-10-25/\n",
      "  inflating: aws/dist/awscli/data/ce/2017-10-25/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/lex-runtime/2016-11-28/\n",
      "  inflating: aws/dist/awscli/data/lex-runtime/2016-11-28/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/mediastore-data/2017-09-01/\n",
      "  inflating: aws/dist/awscli/data/mediastore-data/2017-09-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/stepfunctions/2016-11-23/\n",
      "  inflating: aws/dist/awscli/data/stepfunctions/2016-11-23/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/es/2015-01-01/\n",
      "  inflating: aws/dist/awscli/data/es/2015-01-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/ses/2010-12-01/\n",
      "  inflating: aws/dist/awscli/data/ses/2010-12-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/opsworkscm/2016-11-01/\n",
      "  inflating: aws/dist/awscli/data/opsworkscm/2016-11-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/sts/2011-06-15/\n",
      "  inflating: aws/dist/awscli/data/sts/2011-06-15/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/workspaces/2015-04-08/\n",
      "  inflating: aws/dist/awscli/data/workspaces/2015-04-08/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/fms/2018-01-01/\n",
      "  inflating: aws/dist/awscli/data/fms/2018-01-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/marketplace-entitlement/2017-01-11/\n",
      "  inflating: aws/dist/awscli/data/marketplace-entitlement/2017-01-11/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/athena/2017-05-18/\n",
      "  inflating: aws/dist/awscli/data/athena/2017-05-18/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/lex-models/2017-04-19/\n",
      "  inflating: aws/dist/awscli/data/lex-models/2017-04-19/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/mediastore/2017-09-01/\n",
      "  inflating: aws/dist/awscli/data/mediastore/2017-09-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/logs/2014-03-28/\n",
      "  inflating: aws/dist/awscli/data/logs/2014-03-28/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/cloudsearchdomain/2013-01-01/\n",
      "  inflating: aws/dist/awscli/data/cloudsearchdomain/2013-01-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/ecr/2015-09-21/\n",
      "  inflating: aws/dist/awscli/data/ecr/2015-09-21/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/workmail/2017-10-01/\n",
      "  inflating: aws/dist/awscli/data/workmail/2017-10-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/xray/2016-04-12/\n",
      "  inflating: aws/dist/awscli/data/xray/2016-04-12/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/macie/2017-12-19/\n",
      "  inflating: aws/dist/awscli/data/macie/2017-12-19/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/pinpoint/2016-12-01/\n",
      "  inflating: aws/dist/awscli/data/pinpoint/2016-12-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/organizations/2016-11-28/\n",
      "  inflating: aws/dist/awscli/data/organizations/2016-11-28/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/sagemaker-runtime/2017-05-13/\n",
      "  inflating: aws/dist/awscli/data/sagemaker-runtime/2017-05-13/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/cloudwatch/2010-08-01/\n",
      "  inflating: aws/dist/awscli/data/cloudwatch/2010-08-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/ds/2015-04-16/\n",
      "  inflating: aws/dist/awscli/data/ds/2015-04-16/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/gamelift/2015-10-01/\n",
      "  inflating: aws/dist/awscli/data/gamelift/2015-10-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/autoscaling/2011-01-01/\n",
      "  inflating: aws/dist/awscli/data/autoscaling/2011-01-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/cloudformation/2010-05-15/\n",
      "  inflating: aws/dist/awscli/data/cloudformation/2010-05-15/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/iot1click-devices/2018-05-14/\n",
      "  inflating: aws/dist/awscli/data/iot1click-devices/2018-05-14/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/servicediscovery/2017-03-14/\n",
      "  inflating: aws/dist/awscli/data/servicediscovery/2017-03-14/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/appsync/2017-07-25/\n",
      "  inflating: aws/dist/awscli/data/appsync/2017-07-25/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/translate/2017-07-01/\n",
      "  inflating: aws/dist/awscli/data/translate/2017-07-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/eks/2017-11-01/\n",
      "  inflating: aws/dist/awscli/data/eks/2017-11-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/iot-data/2015-05-28/\n",
      "  inflating: aws/dist/awscli/data/iot-data/2015-05-28/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/servicecatalog/2015-12-10/\n",
      "  inflating: aws/dist/awscli/data/servicecatalog/2015-12-10/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/resource-groups/2017-11-27/\n",
      "  inflating: aws/dist/awscli/data/resource-groups/2017-11-27/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/snowball/2016-06-30/\n",
      "  inflating: aws/dist/awscli/data/snowball/2016-06-30/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/cloudhsmv2/2017-04-28/\n",
      "  inflating: aws/dist/awscli/data/cloudhsmv2/2017-04-28/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/lambda/2015-03-31/\n",
      "  inflating: aws/dist/awscli/data/lambda/2015-03-31/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/cur/2017-01-06/\n",
      "  inflating: aws/dist/awscli/data/cur/2017-01-06/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/codepipeline/2015-07-09/\n",
      "  inflating: aws/dist/awscli/data/codepipeline/2015-07-09/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/devicefarm/2015-06-23/\n",
      "  inflating: aws/dist/awscli/data/devicefarm/2015-06-23/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/config/2014-11-12/\n",
      "  inflating: aws/dist/awscli/data/config/2014-11-12/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/kinesis-video-media/2017-09-30/\n",
      "  inflating: aws/dist/awscli/data/kinesis-video-media/2017-09-30/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/importexport/2010-06-01/\n",
      "  inflating: aws/dist/awscli/data/importexport/2010-06-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/cognito-idp/2016-04-18/\n",
      "  inflating: aws/dist/awscli/data/cognito-idp/2016-04-18/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/waf-regional/2016-11-28/\n",
      "  inflating: aws/dist/awscli/data/waf-regional/2016-11-28/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/sagemaker/2017-07-24/\n",
      "  inflating: aws/dist/awscli/data/sagemaker/2017-07-24/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/iam/2010-05-08/\n",
      "  inflating: aws/dist/awscli/data/iam/2010-05-08/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/ssm/2014-11-06/\n",
      "  inflating: aws/dist/awscli/data/ssm/2014-11-06/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/glue/2017-03-31/\n",
      "  inflating: aws/dist/awscli/data/glue/2017-03-31/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/pi/2018-02-27/\n",
      "  inflating: aws/dist/awscli/data/pi/2018-02-27/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/elasticbeanstalk/2010-12-01/\n",
      "  inflating: aws/dist/awscli/data/elasticbeanstalk/2010-12-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/elastictranscoder/2012-09-25/\n",
      "  inflating: aws/dist/awscli/data/elastictranscoder/2012-09-25/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/marketplacecommerceanalytics/2015-07-01/\n",
      "  inflating: aws/dist/awscli/data/marketplacecommerceanalytics/2015-07-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/machinelearning/2014-12-12/\n",
      "  inflating: aws/dist/awscli/data/machinelearning/2014-12-12/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/mq/2017-11-27/\n",
      "  inflating: aws/dist/awscli/data/mq/2017-11-27/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/elbv2/2015-12-01/\n",
      "  inflating: aws/dist/awscli/data/elbv2/2015-12-01/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/comprehend/2017-11-27/\n",
      "  inflating: aws/dist/awscli/data/comprehend/2017-11-27/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/iotanalytics/2017-11-27/\n",
      "  inflating: aws/dist/awscli/data/iotanalytics/2017-11-27/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/lightsail/2016-11-28/\n",
      "  inflating: aws/dist/awscli/data/lightsail/2016-11-28/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/serverlessrepo/2017-09-08/\n",
      "  inflating: aws/dist/awscli/data/serverlessrepo/2017-09-08/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/support/2013-04-15/\n",
      "  inflating: aws/dist/awscli/data/support/2013-04-15/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/sms/2016-10-24/\n",
      "  inflating: aws/dist/awscli/data/sms/2016-10-24/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/kinesis-video-archived-media/2017-09-30/\n",
      "  inflating: aws/dist/awscli/data/kinesis-video-archived-media/2017-09-30/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/cloudhsm/2014-05-30/\n",
      "  inflating: aws/dist/awscli/data/cloudhsm/2014-05-30/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/acm/2015-12-08/\n",
      "  inflating: aws/dist/awscli/data/acm/2015-12-08/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/batch/2016-08-10/\n",
      "  inflating: aws/dist/awscli/data/batch/2016-08-10/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/iot-jobs-data/2017-09-29/\n",
      "  inflating: aws/dist/awscli/data/iot-jobs-data/2017-09-29/completions-1.json  \n",
      "   creating: aws/dist/awscli/data/datapipeline/2012-10-29/\n",
      "  inflating: aws/dist/awscli/data/datapipeline/2012-10-29/completions-1.json  \n",
      "   creating: aws/dist/awscli/customizations/wizard/\n",
      "   creating: aws/dist/awscli/customizations/wizard/wizards/\n",
      "   creating: aws/dist/awscli/customizations/wizard/wizards/configure/\n",
      "   creating: aws/dist/awscli/customizations/wizard/wizards/dynamodb/\n",
      "   creating: aws/dist/awscli/customizations/wizard/wizards/events/\n",
      "   creating: aws/dist/awscli/customizations/wizard/wizards/iam/\n",
      "   creating: aws/dist/awscli/customizations/wizard/wizards/lambda/\n",
      "  inflating: aws/dist/awscli/customizations/wizard/wizards/iam/new-role.yml  \n",
      "  inflating: aws/dist/awscli/customizations/wizard/wizards/lambda/new-function.yml  \n",
      "  inflating: aws/dist/awscli/customizations/wizard/wizards/events/new-rule.yml  \n",
      "  inflating: aws/dist/awscli/customizations/wizard/wizards/configure/_main.yml  \n",
      "  inflating: aws/dist/awscli/customizations/wizard/wizards/dynamodb/new-table.yml  \n",
      "  inflating: aws/dist/awscli/topics/return-codes.rst  \n",
      "  inflating: aws/dist/awscli/topics/s3-faq.rst  \n",
      "  inflating: aws/dist/awscli/topics/config-vars.rst  \n",
      "  inflating: aws/dist/awscli/topics/s3-config.rst  \n",
      "  inflating: aws/dist/awscli/topics/ddb-expressions.rst  \n",
      "  inflating: aws/dist/awscli/topics/topic-tags.json  \n",
      "   creating: aws/dist/docutils/parsers/\n",
      "   creating: aws/dist/docutils/writers/\n",
      "   creating: aws/dist/docutils/writers/html4css1/\n",
      "   creating: aws/dist/docutils/writers/html5_polyglot/\n",
      "   creating: aws/dist/docutils/writers/latex2e/\n",
      "   creating: aws/dist/docutils/writers/odf_odt/\n",
      "   creating: aws/dist/docutils/writers/pep_html/\n",
      "   creating: aws/dist/docutils/writers/s5_html/\n",
      "  inflating: aws/dist/docutils/writers/html5_polyglot/plain.css  \n",
      "  inflating: aws/dist/docutils/writers/html5_polyglot/template.txt  \n",
      "  inflating: aws/dist/docutils/writers/html5_polyglot/tuftig.css  \n",
      "  inflating: aws/dist/docutils/writers/html5_polyglot/minimal.css  \n",
      "  inflating: aws/dist/docutils/writers/html5_polyglot/responsive.css  \n",
      "  inflating: aws/dist/docutils/writers/html5_polyglot/math.css  \n",
      "  inflating: aws/dist/docutils/writers/latex2e/titlingpage.tex  \n",
      "  inflating: aws/dist/docutils/writers/latex2e/docutils.sty  \n",
      "  inflating: aws/dist/docutils/writers/latex2e/titlepage.tex  \n",
      "  inflating: aws/dist/docutils/writers/latex2e/default.tex  \n",
      "  inflating: aws/dist/docutils/writers/latex2e/xelatex.tex  \n",
      "   creating: aws/dist/docutils/writers/s5_html/themes/\n",
      "   creating: aws/dist/docutils/writers/s5_html/themes/big-black/\n",
      "   creating: aws/dist/docutils/writers/s5_html/themes/big-white/\n",
      "   creating: aws/dist/docutils/writers/s5_html/themes/default/\n",
      "   creating: aws/dist/docutils/writers/s5_html/themes/medium-black/\n",
      "   creating: aws/dist/docutils/writers/s5_html/themes/medium-white/\n",
      "   creating: aws/dist/docutils/writers/s5_html/themes/small-black/\n",
      "   creating: aws/dist/docutils/writers/s5_html/themes/small-white/\n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/README.txt  \n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/default/pretty.css  \n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/default/slides.js  \n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/default/print.css  \n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/default/s5-core.css  \n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/default/slides.css  \n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/default/framing.css  \n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/default/opera.css  \n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/default/outline.css  \n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/big-black/pretty.css  \n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/big-black/framing.css  \n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/big-black/__base__  \n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/big-white/framing.css  \n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/big-white/pretty.css  \n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/medium-white/framing.css  \n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/medium-white/pretty.css  \n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/small-black/pretty.css  \n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/small-black/__base__  \n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/medium-black/__base__  \n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/medium-black/pretty.css  \n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/small-white/pretty.css  \n",
      "  inflating: aws/dist/docutils/writers/s5_html/themes/small-white/framing.css  \n",
      "  inflating: aws/dist/docutils/writers/odf_odt/styles.odt  \n",
      "  inflating: aws/dist/docutils/writers/html4css1/template.txt  \n",
      "  inflating: aws/dist/docutils/writers/html4css1/html4css1.css  \n",
      "  inflating: aws/dist/docutils/writers/pep_html/pep.css  \n",
      "  inflating: aws/dist/docutils/writers/pep_html/template.txt  \n",
      "   creating: aws/dist/docutils/parsers/rst/\n",
      "   creating: aws/dist/docutils/parsers/rst/include/\n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isoamsb.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isomfrk.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isogrk1.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isolat1.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isoamsn.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isopub.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isomopf-wide.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isomfrk-wide.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/xhtml1-special.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isomopf.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/mmlalias.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isobox.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isoamsa.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isoamso.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isogrk4.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isolat2.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isocyr2.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isogrk4-wide.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isogrk2.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isodia.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isonum.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/xhtml1-lat1.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/mmlextra.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isocyr1.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/s5defs.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isomscr.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isotech.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isogrk3.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isomscr-wide.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isoamsc.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/isoamsr.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/README.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/mmlextra-wide.txt  \n",
      "  inflating: aws/dist/docutils/parsers/rst/include/xhtml1-symbol.txt  \n",
      "You can now run: /usr/local/bin/aws --version\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "unzip awscliv2.zip\n",
    "sudo ./aws/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41ad03f8-7e1c-4f43-b35d-142623c69339",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i-0c04b5f612d8e65e9\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "InstanceId=`wget -q -O - http://169.254.169.254/latest/meta-data/instance-id`\n",
    "echo $InstanceId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e262c78f-354f-452a-95ae-9e64fb630b89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (UnauthorizedOperation) when calling the DescribeTags operation: You are not authorized to perform this operation.\n"
     ]
    }
   ],
   "source": [
    "%sh aws ec2 describe-tags --filters \"Name=resource-id,Values=i-0c04b5f612d8e65e9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7b91ae5-36ea-4dce-aef2-293633f87124",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "$sh\n",
    "if [ -z $1 ]; then\n",
    "    scriptName=`basename \"$0\"`\n",
    "    echo  >&2 \"Usage: $scriptName <tag_name>\"\n",
    "    exit 1\n",
    "fi\n",
    " \n",
    "# check that aws and ec2-metadata commands are installed\n",
    "command -v aws >/dev/null 2>&1 || { echo >&2 'aws command not installed.'; exit 2; }\n",
    "command -v ec2-metadata >/dev/null 2>&1 || { echo >&2 'ec2-metadata command not installed.'; exit 3; }\n",
    " \n",
    "# set filter parameters\n",
    "instanceId=$(ec2-metadata -i | cut -d ' ' -f2)\n",
    "filterParams=( --filters \"Name=key,Values=$1\" \"Name=resource-type,Values=instance\" \"Name=resource-id,Values=$instanceId\" )\n",
    " \n",
    "# get region\n",
    "region=$(ec2-metadata --availability-zone | cut -d ' ' -f2)\n",
    "region=${region%?}\n",
    " \n",
    "# retrieve tags\n",
    "tagValues=$(aws ec2 describe-tags --output text --region \"$region\" \"${filterParams[@]}\")\n",
    "if [ $? -ne 0 ]; then\n",
    "    echo >&2 \"Error retrieving tag value.\"\n",
    "    exit 4\n",
    "fi\n",
    " \n",
    "# extract required tag value\n",
    "tagValue=$(echo \"$tagValues\" | cut -f5)\n",
    "echo \"$tagValue\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "225ad148-fce4-4bbe-8b2e-a2a7a6c5c824",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   337  100   337    0     0   329k      0 --:--:-- --:--:-- --:--:--  329k\n",
      "<?xml version=\"1.0\" encoding=\"iso-8859-1\"?>\n",
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
      "\t\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n",
      " <head>\n",
      "  <title>404 - Not Found</title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>404 - Not Found</h1>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "curl  http://169.254.169.254/latest/meta-data/tags/instance/ClusterName\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d93b7586-6394-486f-970b-0a7bbc5f62fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100    56  100    56    0     0  56000      0 --:--:-- --:--:-- --:--:-- 56000\n",
      "*   Trying 169.254.169.254:80...\n",
      "* TCP_NODELAY set\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0* Connected to 169.254.169.254 (169.254.169.254) port 80 (#0)\n",
      "> GET /latest/meta-data/tags/instance/ClusterName HTTP/1.1\n",
      "> Host: 169.254.169.254\n",
      "> User-Agent: curl/7.68.0\n",
      "> Accept: */*\n",
      "> X-aws-ec2-metadata-token: AQAAAGgjImbM2kcMbXLuxDhCwOIEMj2Nnm5kgWvQXBpzFokNEpjTWA==\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "* HTTP 1.0, assume close after body\n",
      "< HTTP/1.0 404 Not Found\n",
      "< Content-Length: 337\n",
      "< Content-Type: text/html\n",
      "< Date: Wed, 26 Apr 2023 22:45:07 GMT\n",
      "< X-Aws-Ec2-Metadata-Token-Ttl-Seconds: 21600\n",
      "< Connection: close\n",
      "< Server: EC2ws\n",
      "< \n",
      "{ [337 bytes data]\n",
      "\n",
      "100   337  100   337    0     0   329k      0 --:--:-- --:--:-- --:--:--  329k\n",
      "* Closing connection 0\n",
      "<?xml version=\"1.0\" encoding=\"iso-8859-1\"?>\n",
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
      "\t\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n",
      " <head>\n",
      "  <title>404 - Not Found</title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>404 - Not Found</h1>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "TOKEN=`curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"` \\\n",
    "&& curl -H \"X-aws-ec2-metadata-token: $TOKEN\" -v http://169.254.169.254/latest/meta-data/tags/instance/ClusterName\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0d8e75d-0869-4efe-ac44-e6427f6d79d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db-test\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "cat /tmp/cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e2dd9d0-d2a7-4454-afe0-fbf7650faf87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "echo $DB_CLUSTER_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94c03835-15b4-4e12-aaec-d92281d41e9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15946b29-f14d-476c-abdb-fef407a3badf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILD\n",
      "BUILDINFO\n",
      "DBR_VERSION\n",
      "chauffeur\n",
      "chauffeur-jars\n",
      "common\n",
      "conda\n",
      "conf_reader_version.bzl\n",
      "data\n",
      "databricks-hive\n",
      "devbricks\n",
      "driver\n",
      "executor\n",
      "ganglia-api\n",
      "ganglia-api-cbe773d051168e05118774708ff7a0ce881617f4.tar.gz\n",
      "glue\n",
      "hadoop-safety-jars\n",
      "hive\n",
      "init_scripts\n",
      "jars\n",
      "jdkpatch\n",
      "kernel-connections\n",
      "keys\n",
      "licenses\n",
      "miniconda\n",
      "monit-5.30.0\n",
      "python\n",
      "python-bootstrap\n",
      "python3\n",
      "python_shell\n",
      "safespark\n",
      "scala-kernel-jars\n",
      "simbasparkodbc_1.0.4-2_amd64.deb\n",
      "spark\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "ls /databricks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c88bcb75-7004-4d25-ad6a-0140a4a303f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n",
       "\u001b[0;32m<command-394056956140861>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n",
       "\u001b[1;32m      5\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m----> 7\u001b[0;31m   \u001b[0m_sqldf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m____databricks_percent_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      9\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0m____databricks_percent_sql\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m<command-394056956140861>\u001b[0m in \u001b[0;36m____databricks_percent_sql\u001b[0;34m()\u001b[0m\n",
       "\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m____databricks_percent_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      3\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandard_b64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Q1JFQVRFIFRBQkxFIGhpdmVfbWV0YXN0b3JlLnIzNjBfdXRpbHMuYWNsX2FycF9wb2xpY2llcyAoCmBDT0wxYCBTVFJJTkcsCmBDT0wyYCBTVFJJTkcsCmBDT0wzYCBTVFJJTkcpClVTSU5HIEpEQkMKT1BUSU9OUyAoCmB1cmxgICdqZGJjOm9yYWNsZTp0aGluOi8veGluLm9yYWNsZS5jb20nLApgZHJpdmVyYCAnb3JhY2xlLmpkYmMuZHJpdmVyLk9yYWNsZURyaXZlcicsCmBmZXRjaHNpemVgICcxMDAwMCcsCmBkYnRhYmxlYCAnKFNlbGVjdCBDT0wxLENPTDIsQ09MMwpmcm9tIGNhcHRhaW4pIHRtcCcsCmB1c2VyYCAneGluJywKYHBhc3N3b3JkYCAnaGVoZScp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n",
       "\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n",
       "\u001b[1;32m   1117\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   1118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m-> 1119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m   1120\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   1121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n",
       "\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m--> 196\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    198\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
       "\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n",
       "\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
       "\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o343.sql.\n",
       ": java.lang.ClassNotFoundException: oracle.jdbc.driver.OracleDriver\n",
       "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n",
       "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:419)\n",
       "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:352)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:36)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:392)\n",
       "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:83)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:78)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:89)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:229)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:243)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:392)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:188)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:142)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:342)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:229)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:214)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:227)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:220)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:99)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:298)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:294)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:220)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:354)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:220)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:174)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:165)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:238)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:107)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:104)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:820)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:815)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n\u001b[0;32m<command-394056956140861>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0m_sqldf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m____databricks_percent_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0m____databricks_percent_sql\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m<command-394056956140861>\u001b[0m in \u001b[0;36m____databricks_percent_sql\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m____databricks_percent_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandard_b64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Q1JFQVRFIFRBQkxFIGhpdmVfbWV0YXN0b3JlLnIzNjBfdXRpbHMuYWNsX2FycF9wb2xpY2llcyAoCmBDT0wxYCBTVFJJTkcsCmBDT0wyYCBTVFJJTkcsCmBDT0wzYCBTVFJJTkcpClVTSU5HIEpEQkMKT1BUSU9OUyAoCmB1cmxgICdqZGJjOm9yYWNsZTp0aGluOi8veGluLm9yYWNsZS5jb20nLApgZHJpdmVyYCAnb3JhY2xlLmpkYmMuZHJpdmVyLk9yYWNsZURyaXZlcicsCmBmZXRjaHNpemVgICcxMDAwMCcsCmBkYnRhYmxlYCAnKFNlbGVjdCBDT0wxLENPTDIsQ09MMwpmcm9tIGNhcHRhaW4pIHRtcCcsCmB1c2VyYCAneGluJywKYHBhc3N3b3JkYCAnaGVoZScp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n\n\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o343.sql.\n: java.lang.ClassNotFoundException: oracle.jdbc.driver.OracleDriver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:419)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:352)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:36)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:392)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:83)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:78)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:89)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:229)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:243)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:392)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:188)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:342)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:229)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:214)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:227)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:220)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:99)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:294)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:220)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:354)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:220)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:174)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:165)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:238)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:107)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:104)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:815)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "java.lang.ClassNotFoundException: oracle.jdbc.driver.OracleDriver",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE TABLE hive_metastore.r360_utils.acl_arp_policies (\n",
    "`COL1` STRING,\n",
    "`COL2` STRING,\n",
    "`COL3` STRING)\n",
    "USING JDBC\n",
    "OPTIONS (\n",
    "`url` 'jdbc:oracle:thin://xin.oracle.com',\n",
    "`driver` 'oracle.jdbc.driver.OracleDriver',\n",
    "`fetchsize` '10000',\n",
    "`dbtable` '(Select COL1,COL2,COL3\n",
    "from captain) tmp',\n",
    "`user` 'xin',\n",
    "`password` 'hehe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcd7e059-9912-4300-aa71-efcd85357724",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>tableName</th><th>isTemporary</th></tr></thead><tbody><tr><td>default</td><td>abc</td><td>false</td></tr><tr><td>default</td><td>accountbillableusage</td><td>false</td></tr><tr><td>default</td><td>accounts</td><td>false</td></tr><tr><td>default</td><td>accounts_dlt</td><td>false</td></tr><tr><td>default</td><td>accountsmanager</td><td>false</td></tr><tr><td>default</td><td>akash_ext_table</td><td>false</td></tr><tr><td>default</td><td>akash_ext_table1</td><td>false</td></tr><tr><td>default</td><td>boxes</td><td>false</td></tr><tr><td>default</td><td>bronze</td><td>false</td></tr><tr><td>default</td><td>cache_test_1</td><td>false</td></tr><tr><td>default</td><td>carsflat</td><td>false</td></tr><tr><td>default</td><td>catalogs</td><td>false</td></tr><tr><td>default</td><td>clusters</td><td>false</td></tr><tr><td>default</td><td>customer</td><td>false</td></tr><tr><td>default</td><td>dbr_support_prasad2</td><td>false</td></tr><tr><td>default</td><td>dbr_test_ascii</td><td>false</td></tr><tr><td>default</td><td>dd_view</td><td>false</td></tr><tr><td>default</td><td>decimal_field</td><td>false</td></tr><tr><td>default</td><td>decimal_field1</td><td>false</td></tr><tr><td>default</td><td>delta_events_1_1</td><td>false</td></tr><tr><td>default</td><td>delta_events_2</td><td>false</td></tr><tr><td>default</td><td>delta_events_3</td><td>false</td></tr><tr><td>default</td><td>delta_external_events_1</td><td>false</td></tr><tr><td>default</td><td>delta_external_events_2</td><td>false</td></tr><tr><td>default</td><td>delta_external_events_3</td><td>false</td></tr><tr><td>default</td><td>delta_external_events_sink_1</td><td>false</td></tr><tr><td>default</td><td>delta_external_events_sink_2</td><td>false</td></tr><tr><td>default</td><td>delta_external_events_sink_3</td><td>false</td></tr><tr><td>default</td><td>department</td><td>false</td></tr><tr><td>default</td><td>department_external_table_csv</td><td>false</td></tr><tr><td>default</td><td>department_external_table_parquet</td><td>false</td></tr><tr><td>default</td><td>department_external_table_v1</td><td>false</td></tr><tr><td>default</td><td>department_external_view</td><td>false</td></tr><tr><td>default</td><td>department_managed_v1</td><td>false</td></tr><tr><td>default</td><td>department_non_admin</td><td>false</td></tr><tr><td>default</td><td>diamonds</td><td>false</td></tr><tr><td>default</td><td>doubleverify</td><td>false</td></tr><tr><td>default</td><td>dpt_test</td><td>false</td></tr><tr><td>default</td><td>dtenedor_demo</td><td>false</td></tr><tr><td>default</td><td>experienced_employee</td><td>false</td></tr><tr><td>default</td><td>external_loc_test_v1</td><td>false</td></tr><tr><td>default</td><td>external_table_test1</td><td>false</td></tr><tr><td>default</td><td>globalinitscripts</td><td>false</td></tr><tr><td>default</td><td>hii</td><td>false</td></tr><tr><td>default</td><td>hiii</td><td>false</td></tr><tr><td>default</td><td>ming_test</td><td>false</td></tr><tr><td>default</td><td>my_table</td><td>false</td></tr><tr><td>default</td><td>my_table_1</td><td>false</td></tr><tr><td>default</td><td>notebook</td><td>false</td></tr><tr><td>default</td><td>oracle_string_test</td><td>false</td></tr><tr><td>default</td><td>partiition_test_1</td><td>false</td></tr><tr><td>default</td><td>partitioned</td><td>false</td></tr><tr><td>default</td><td>people_10m</td><td>false</td></tr><tr><td>default</td><td>phones_1</td><td>false</td></tr><tr><td>default</td><td>phones_2</td><td>false</td></tr><tr><td>default</td><td>query1</td><td>false</td></tr><tr><td>default</td><td>query2</td><td>false</td></tr><tr><td>default</td><td>quickstart_table</td><td>false</td></tr><tr><td>default</td><td>raw_documentation1</td><td>false</td></tr><tr><td>default</td><td>sales1</td><td>false</td></tr><tr><td>default</td><td>silver</td><td>false</td></tr><tr><td>default</td><td>source</td><td>false</td></tr><tr><td>default</td><td>sqlpermissions</td><td>false</td></tr><tr><td>default</td><td>stores1</td><td>false</td></tr><tr><td>default</td><td>student</td><td>false</td></tr><tr><td>default</td><td>studentst</td><td>false</td></tr><tr><td>default</td><td>swaroop_test</td><td>false</td></tr><tr><td>default</td><td>table2main</td><td>false</td></tr><tr><td>default</td><td>table46</td><td>false</td></tr><tr><td>default</td><td>table_job_main</td><td>false</td></tr><tr><td>default</td><td>table_that_will_replace_other_tables</td><td>false</td></tr><tr><td>default</td><td>table_vvn_1</td><td>false</td></tr><tr><td>default</td><td>tabletm</td><td>false</td></tr><tr><td>default</td><td>target_partitoned</td><td>false</td></tr><tr><td>default</td><td>test</td><td>false</td></tr><tr><td>default</td><td>test1</td><td>false</td></tr><tr><td>default</td><td>test_array_upstart</td><td>false</td></tr><tr><td>default</td><td>test_csv</td><td>false</td></tr><tr><td>default</td><td>test_history</td><td>false</td></tr><tr><td>default</td><td>test_table</td><td>false</td></tr><tr><td>default</td><td>test_table2</td><td>false</td></tr><tr><td>default</td><td>test_table3</td><td>false</td></tr><tr><td>default</td><td>test_view</td><td>false</td></tr><tr><td>default</td><td>testdeltatable</td><td>false</td></tr><tr><td>default</td><td>testing123</td><td>false</td></tr><tr><td>default</td><td>testing123_1</td><td>false</td></tr><tr><td>default</td><td>testtable</td><td>false</td></tr><tr><td>default</td><td>tmtest</td><td>false</td></tr><tr><td>default</td><td>tof_orx_mbr_enrich</td><td>false</td></tr><tr><td>default</td><td>uc_hive_combined_view</td><td>false</td></tr><tr><td>default</td><td>uc_merge_tbl</td><td>false</td></tr><tr><td>default</td><td>uc_merge_tbl_updates</td><td>false</td></tr><tr><td>default</td><td>uc_partitioned_parquet_external</td><td>false</td></tr><tr><td>default</td><td>uc_partitioned_tbl</td><td>false</td></tr><tr><td>default</td><td>uc_view_source_table_a</td><td>false</td></tr><tr><td>default</td><td>uc_view_source_table_b</td><td>false</td></tr><tr><td>default</td><td>uc_view_test</td><td>false</td></tr><tr><td>default</td><td>uc_view_transitivity_test</td><td>false</td></tr><tr><td>default</td><td>uniform_demo_table</td><td>false</td></tr><tr><td>default</td><td>unitycatalog</td><td>false</td></tr><tr><td>default</td><td>view_name1127</td><td>false</td></tr><tr><td>default</td><td>view_name1127_1</td><td>false</td></tr><tr><td>default</td><td>view_name1127_2</td><td>false</td></tr><tr><td>default</td><td>view_name1127_3</td><td>false</td></tr><tr><td>default</td><td>view_name1128</td><td>false</td></tr><tr><td>default</td><td>view_vvn_001</td><td>false</td></tr><tr><td>default</td><td>view_vvn_1</td><td>false</td></tr><tr><td>default</td><td>vvn_dynamic</td><td>false</td></tr><tr><td>default</td><td>vvn_report_1_pivot</td><td>false</td></tr><tr><td>default</td><td>vvn_report_1_unformated</td><td>false</td></tr><tr><td>default</td><td>workspace</td><td>false</td></tr><tr><td>default</td><td>z_monitoring_tracker</td><td>false</td></tr><tr><td>default</td><td>z_test</td><td>false</td></tr><tr><td>default</td><td>z_test__1</td><td>false</td></tr><tr><td>default</td><td>z_test__10</td><td>false</td></tr><tr><td>default</td><td>z_test__2</td><td>false</td></tr><tr><td>default</td><td>z_test__3</td><td>false</td></tr><tr><td>default</td><td>z_test__4</td><td>false</td></tr><tr><td>default</td><td>z_test__5</td><td>false</td></tr><tr><td>default</td><td>z_test__6</td><td>false</td></tr><tr><td>default</td><td>z_test__7</td><td>false</td></tr><tr><td>default</td><td>z_test__8</td><td>false</td></tr><tr><td>default</td><td>z_test__9</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "default",
         "abc",
         false
        ],
        [
         "default",
         "accountbillableusage",
         false
        ],
        [
         "default",
         "accounts",
         false
        ],
        [
         "default",
         "accounts_dlt",
         false
        ],
        [
         "default",
         "accountsmanager",
         false
        ],
        [
         "default",
         "akash_ext_table",
         false
        ],
        [
         "default",
         "akash_ext_table1",
         false
        ],
        [
         "default",
         "boxes",
         false
        ],
        [
         "default",
         "bronze",
         false
        ],
        [
         "default",
         "cache_test_1",
         false
        ],
        [
         "default",
         "carsflat",
         false
        ],
        [
         "default",
         "catalogs",
         false
        ],
        [
         "default",
         "clusters",
         false
        ],
        [
         "default",
         "customer",
         false
        ],
        [
         "default",
         "dbr_support_prasad2",
         false
        ],
        [
         "default",
         "dbr_test_ascii",
         false
        ],
        [
         "default",
         "dd_view",
         false
        ],
        [
         "default",
         "decimal_field",
         false
        ],
        [
         "default",
         "decimal_field1",
         false
        ],
        [
         "default",
         "delta_events_1_1",
         false
        ],
        [
         "default",
         "delta_events_2",
         false
        ],
        [
         "default",
         "delta_events_3",
         false
        ],
        [
         "default",
         "delta_external_events_1",
         false
        ],
        [
         "default",
         "delta_external_events_2",
         false
        ],
        [
         "default",
         "delta_external_events_3",
         false
        ],
        [
         "default",
         "delta_external_events_sink_1",
         false
        ],
        [
         "default",
         "delta_external_events_sink_2",
         false
        ],
        [
         "default",
         "delta_external_events_sink_3",
         false
        ],
        [
         "default",
         "department",
         false
        ],
        [
         "default",
         "department_external_table_csv",
         false
        ],
        [
         "default",
         "department_external_table_parquet",
         false
        ],
        [
         "default",
         "department_external_table_v1",
         false
        ],
        [
         "default",
         "department_external_view",
         false
        ],
        [
         "default",
         "department_managed_v1",
         false
        ],
        [
         "default",
         "department_non_admin",
         false
        ],
        [
         "default",
         "diamonds",
         false
        ],
        [
         "default",
         "doubleverify",
         false
        ],
        [
         "default",
         "dpt_test",
         false
        ],
        [
         "default",
         "dtenedor_demo",
         false
        ],
        [
         "default",
         "experienced_employee",
         false
        ],
        [
         "default",
         "external_loc_test_v1",
         false
        ],
        [
         "default",
         "external_table_test1",
         false
        ],
        [
         "default",
         "globalinitscripts",
         false
        ],
        [
         "default",
         "hii",
         false
        ],
        [
         "default",
         "hiii",
         false
        ],
        [
         "default",
         "ming_test",
         false
        ],
        [
         "default",
         "my_table",
         false
        ],
        [
         "default",
         "my_table_1",
         false
        ],
        [
         "default",
         "notebook",
         false
        ],
        [
         "default",
         "oracle_string_test",
         false
        ],
        [
         "default",
         "partiition_test_1",
         false
        ],
        [
         "default",
         "partitioned",
         false
        ],
        [
         "default",
         "people_10m",
         false
        ],
        [
         "default",
         "phones_1",
         false
        ],
        [
         "default",
         "phones_2",
         false
        ],
        [
         "default",
         "query1",
         false
        ],
        [
         "default",
         "query2",
         false
        ],
        [
         "default",
         "quickstart_table",
         false
        ],
        [
         "default",
         "raw_documentation1",
         false
        ],
        [
         "default",
         "sales1",
         false
        ],
        [
         "default",
         "silver",
         false
        ],
        [
         "default",
         "source",
         false
        ],
        [
         "default",
         "sqlpermissions",
         false
        ],
        [
         "default",
         "stores1",
         false
        ],
        [
         "default",
         "student",
         false
        ],
        [
         "default",
         "studentst",
         false
        ],
        [
         "default",
         "swaroop_test",
         false
        ],
        [
         "default",
         "table2main",
         false
        ],
        [
         "default",
         "table46",
         false
        ],
        [
         "default",
         "table_job_main",
         false
        ],
        [
         "default",
         "table_that_will_replace_other_tables",
         false
        ],
        [
         "default",
         "table_vvn_1",
         false
        ],
        [
         "default",
         "tabletm",
         false
        ],
        [
         "default",
         "target_partitoned",
         false
        ],
        [
         "default",
         "test",
         false
        ],
        [
         "default",
         "test1",
         false
        ],
        [
         "default",
         "test_array_upstart",
         false
        ],
        [
         "default",
         "test_csv",
         false
        ],
        [
         "default",
         "test_history",
         false
        ],
        [
         "default",
         "test_table",
         false
        ],
        [
         "default",
         "test_table2",
         false
        ],
        [
         "default",
         "test_table3",
         false
        ],
        [
         "default",
         "test_view",
         false
        ],
        [
         "default",
         "testdeltatable",
         false
        ],
        [
         "default",
         "testing123",
         false
        ],
        [
         "default",
         "testing123_1",
         false
        ],
        [
         "default",
         "testtable",
         false
        ],
        [
         "default",
         "tmtest",
         false
        ],
        [
         "default",
         "tof_orx_mbr_enrich",
         false
        ],
        [
         "default",
         "uc_hive_combined_view",
         false
        ],
        [
         "default",
         "uc_merge_tbl",
         false
        ],
        [
         "default",
         "uc_merge_tbl_updates",
         false
        ],
        [
         "default",
         "uc_partitioned_parquet_external",
         false
        ],
        [
         "default",
         "uc_partitioned_tbl",
         false
        ],
        [
         "default",
         "uc_view_source_table_a",
         false
        ],
        [
         "default",
         "uc_view_source_table_b",
         false
        ],
        [
         "default",
         "uc_view_test",
         false
        ],
        [
         "default",
         "uc_view_transitivity_test",
         false
        ],
        [
         "default",
         "uniform_demo_table",
         false
        ],
        [
         "default",
         "unitycatalog",
         false
        ],
        [
         "default",
         "view_name1127",
         false
        ],
        [
         "default",
         "view_name1127_1",
         false
        ],
        [
         "default",
         "view_name1127_2",
         false
        ],
        [
         "default",
         "view_name1127_3",
         false
        ],
        [
         "default",
         "view_name1128",
         false
        ],
        [
         "default",
         "view_vvn_001",
         false
        ],
        [
         "default",
         "view_vvn_1",
         false
        ],
        [
         "default",
         "vvn_dynamic",
         false
        ],
        [
         "default",
         "vvn_report_1_pivot",
         false
        ],
        [
         "default",
         "vvn_report_1_unformated",
         false
        ],
        [
         "default",
         "workspace",
         false
        ],
        [
         "default",
         "z_monitoring_tracker",
         false
        ],
        [
         "default",
         "z_test",
         false
        ],
        [
         "default",
         "z_test__1",
         false
        ],
        [
         "default",
         "z_test__10",
         false
        ],
        [
         "default",
         "z_test__2",
         false
        ],
        [
         "default",
         "z_test__3",
         false
        ],
        [
         "default",
         "z_test__4",
         false
        ],
        [
         "default",
         "z_test__5",
         false
        ],
        [
         "default",
         "z_test__6",
         false
        ],
        [
         "default",
         "z_test__7",
         false
        ],
        [
         "default",
         "z_test__8",
         false
        ],
        [
         "default",
         "z_test__9",
         false
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 4
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tableName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isTemporary",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql show tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f4f1236-817e-4389-a90a-fd151365ad55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>deptcode</th><th>deptname</th><th>location</th></tr></thead><tbody><tr><td>1</td><td>dang</td><td>Bang</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "dang",
         "Bang"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 5
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "deptcode",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "deptname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql select * from abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4da9e7d-ce74-4cab-b22c-8eabc6c3685d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql \n",
    "drop table raw_documentation1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d57f6e05-e34f-484b-a075-38946b7b7458",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>deptcode</td><td>int</td><td>null</td></tr><tr><td>deptname</td><td>string</td><td>null</td></tr><tr><td>location</td><td>string</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "deptcode",
         "int",
         null
        ],
        [
         "deptname",
         "string",
         null
        ],
        [
         "location",
         "string",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 7
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"comment\":\"name of the column\"}",
         "name": "col_name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"data type of the column\"}",
         "name": "data_type",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"comment of the column\"}",
         "name": "comment",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql describe table raw_documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b831cd3-c40f-47ae-8051-f4496585cb72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not spark.catalog.tableExists(\"raw_documentation\") or spark.table(\"raw_documentation\").isEmpty():\n",
    "    # Download Databricks documentation to a DataFrame (see _resources/00-init for more details)\n",
    "    doc_articles = spark.table(\"abc\")\n",
    "    #Save them as a raw_documentation table\n",
    "    doc_articles.write.mode('overwrite').saveAsTable(\"raw_documentation\")\n",
    "    # doc_articles.write.mode('overwrite').format(\"delta\").save(\"s3://e1-b-rd-research01/rag_chatbot/delta/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee6dcab7-b9a3-4361-bb68-5040268587df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ls: cannot open directory '/databricks/hive/': Permission denied\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "ls /databricks/hive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5a8e7d5-fef2-4c3a-bc13-27c6386fa109",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Repos\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Repos\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh\n",
    "ls /Workspace/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3243838-3a8b-481c-ab9a-b6efdd61494b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "echo ${DB_CLUSTER_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9ab71d8-88e6-48a2-95f9-640b6e452023",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 2: DB_CLUSTER_ID: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initiating tcp dump\n",
      "initiated tcp dump\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcpdump: listening on eth0, link-type EN10MB (Ethernet), snapshot length 262144 bytes\n",
      "cp: cannot stat '/dbfs/tmp/0915-183002-qq8101k1/.': Not a directory\n",
      "cp: cannot stat '/dbfs/tmp/0915-183002-qq8101k1/.': Not a directory\n",
      "cp: cannot stat '/dbfs/tmp/0915-183002-qq8101k1/.': Not a directory\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh\n",
    "MYIP=$(ip route get 10 | awk '{print $NF;exit}')\n",
    "DB_CLUSTER_ID = \"0915-183002-qq8101k1\"\n",
    "echo \"initiating tcp dump\"\n",
    "sudo tcpdump -w /tmp/trace_%Y_%m_%d_%H_%M_%S_${MYIP}.pcap -W 1000 -G 1800 -K -n host mdpartyyphlhsp.caj77bnxuhme.us-west-2.rds.amazonaws.com &\n",
    "echo \"initiated tcp dump\"\n",
    "cat <<'EOF' >> /tmp/copy_stats.sh\n",
    "#!/bin/bash\n",
    "DB_CLUSTER_ID=$(echo $HOSTNAME | awk -F '-' '{print$1\"-\"$2\"-\"$3}')\n",
    "if [[ ! -d /dbfs/tmp/${DB_CLUSTER_ID} ]] ; then\n",
    "sudo mkdir -p /dbfs/tmp/${DB_CLUSTER_ID}\n",
    "fi\n",
    "BASEDIR=\"/dbfs/tmp/${DB_CLUSTER_ID}\"\n",
    "while [ 1 ]; do\n",
    "sudo cp -f /tmp/*.pcap ${BASEDIR}/.\n",
    "sleep 5\n",
    "done\n",
    "EOF\n",
    "chmod a+x /tmp/copy_stats.sh\n",
    "/tmp/copy_stats.sh & disown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1f8966e-166b-43e1-8cda-ddbc4a3921e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Age</th></tr></thead><tbody><tr><td>David</td><td>32</td></tr><tr><td>Eva</td><td>29</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "David",
         32
        ],
        [
         "Eva",
         29
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 1
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql select * from  hive_metastore.aditya.employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e847d687-95d3-46d6-a0d0-cdcec0141c64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a271510-263a-44b4-bace-6e403c40e692",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<configuration>\n",
      "  \n",
      "<property>\n",
      "  <name>javax.jdo.option.ConnectionURL</name>\n",
      "  <value>jdbc:mariadb://mdpartyyphlhsp.caj77bnxuhme.us-west-2.rds.amazonaws.com:3306/organization4458556048801738?useSSL=true&amp;enabledSslProtocolSuites=TLSv1,TLSv1.1,TLSv1.2&amp;serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt</value>\n",
      "  <description>JDBC connect string for a JDBC metastore</description>\n",
      "</property>\n",
      "<property>\n",
      "  <name>javax.jdo.option.ConnectionDriverName</name>\n",
      "  <value>org.mariadb.jdbc.Driver</value>\n",
      "  <description>Driver class name for a JDBC metastore</description>\n",
      "</property>\n",
      "<property>\n",
      "  <name>javax.jdo.option.ConnectionUserName</name>\n",
      "  <value>[REDACTED]</value>\n",
      "  <description>Username to use against metastore database</description>\n",
      "</property>\n",
      "<property>\n",
      "  <name>javax.jdo.option.ConnectionPassword</name>\n",
      "  <value>[REDACTED]</value>\n",
      "  <description>Password to use against metastore database</description>\n",
      "</property>\n",
      "<!-- If the following two properties are not set correctly, the metastore will\n",
      "     attempt to initialize its schema upon startup\n",
      "-->\n",
      "<property>\n",
      "  <name>datanucleus.schema.autoCreateAll</name>\n",
      "  <value>false</value>\n",
      "</property>\n",
      "<property>\n",
      "  <name>datanucleus.autoCreateSchema</name>\n",
      "  <value>false</value>\n",
      "</property>\n",
      "<property>\n",
      "  <name>datanucleus.fixedDatastore</name>\n",
      "    <value>true</value>\n",
      "</property>\n",
      "<property>\n",
      "  <name>datanucleus.connectionPool.minPoolSize</name>\n",
      "    <value>0</value>\n",
      "</property>\n",
      "<property>\n",
      "  <name>datanucleus.connectionPool.initialPoolSize</name>\n",
      "    <value>0</value>\n",
      "</property>\n",
      "<property>\n",
      "  <name>datanucleus.connectionPool.maxPoolSize</name>\n",
      "    <value>1</value>\n",
      "</property>\n",
      "<property>\n",
      "  <name>hive.stats.autogather</name>\n",
      "  <value>false</value>\n",
      "</property>\n",
      "       \n",
      "  <property>\n",
      "    <name>mapred.reduce.tasks</name>\n",
      "    <value>100</value>\n",
      "  </property>\n",
      "  <!-- To mitigate the problem of PROD-4498 and per HIVE-7140, we need to bump the timeout.\n",
      "       Since the default value of this property used by Impala is 3600 seconds, we will use this value for\n",
      "       actual deployment\n",
      "       (http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cm_props_cdh530_impala.html).\n",
      "  -->\n",
      "  <property>\n",
      "    <name>hive.metastore.client.socket.timeout</name>\n",
      "    <value>3600</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>hadoop.tmp.dir</name>\n",
      "    <value>/local_disk0/tmp</value>\n",
      "  </property>\n",
      "\n",
      "  <property>\n",
      "    <name>hive.metastore.client.connect.retry.delay</name>\n",
      "    <value>10</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>hive.metastore.failure.retries</name>\n",
      "    <value>30</value>\n",
      "  </property>\n",
      "\n",
      "\n",
      "</configuration>\n",
      "     "
     ]
    }
   ],
   "source": [
    "%sh\n",
    "cat /databricks/hive/conf/hive-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e25a36cd-02c8-42d7-87ee-c076e773a08f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 55.8M  100 55.8M    0     0   114M      0 --:--:-- --:--:-- --:--:--  114M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 58568\n",
      "-rw-r--r-- 1 root root 58613990 Oct  5 20:29 awscliv2.zip\n",
      "drwxr-xr-x 2 root root     4096 Oct  1 18:06 azure\n",
      "drwxr-xr-x 2 root root     4096 Oct  1 18:06 conf\n",
      "drwxr-xr-x 4 root root     4096 Oct  2 07:05 eventlogs\n",
      "-r-xr-xr-x 1 root root     2755 Oct  1 18:06 hadoop_accessed_config.lst\n",
      "drwxr-xr-x 2 root root    20480 Oct  5 20:26 logs\n",
      "-r-xr-xr-x 1 root root  1306936 Oct  1 18:06 preload_class.lst\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "ls -l\n",
    "#unzip awscliv2.zip\n",
    "#sudo ./aws/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a6f71d5-8a0f-4f12-a79f-a60a338f2def",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>tableName</th><th>isTemporary</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tableName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isTemporary",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "show tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea41d960-2496-4914-bff8-2d1eff287b4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>catalog</th></tr></thead><tbody><tr><td>deltasharing</td></tr><tr><td>dev_catalog</td></tr><tr><td>hive_metastore</td></tr><tr><td>main</td></tr><tr><td>quickstart_catalog</td></tr><tr><td>samples</td></tr><tr><td>system</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "deltasharing"
        ],
        [
         "dev_catalog"
        ],
        [
         "hive_metastore"
        ],
        [
         "main"
        ],
        [
         "quickstart_catalog"
        ],
        [
         "samples"
        ],
        [
         "system"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "catalog",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql show catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7d6edb6-47b5-45d1-bfc8-60b0e551e5e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aebe0e61-ae0a-45cc-b060-84f95c96f26d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n",
       "\u001b[0;32m<command-607812982174359>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n",
       "\u001b[1;32m      5\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m----> 7\u001b[0;31m   \u001b[0m_sqldf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m____databricks_percent_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      9\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0m____databricks_percent_sql\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m<command-607812982174359>\u001b[0m in \u001b[0;36m____databricks_percent_sql\u001b[0;34m()\u001b[0m\n",
       "\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m____databricks_percent_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      3\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandard_b64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"R1JBTlQgU0VMRUNUIE9OIENBVEFMT0cgbWFpbiBUTyBtYWluZ3JvdXA=\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n",
       "\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n",
       "\u001b[1;32m   1117\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   1118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m-> 1119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m   1120\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   1121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n",
       "\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m--> 196\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    198\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
       "\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n",
       "\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
       "\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o349.sql.\n",
       ": com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User is not an owner of Catalog 'main'.\n",
       "\tat com.databricks.managedcatalog.UCReliableHttpClient.reliablyAndTranslateExceptions(UCReliableHttpClient.scala:47)\n",
       "\tat com.databricks.managedcatalog.UCReliableHttpClient.patchJson(UCReliableHttpClient.scala:114)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$updatePermissions$1(ManagedCatalogClientImpl.scala:1806)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapException$1(ManagedCatalogClientImpl.scala:2913)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:25)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:23)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:78)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:2912)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.updatePermissions(ManagedCatalogClientImpl.scala:1801)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.addPermissions(ManagedCatalogCommon.scala:702)\n",
       "\tat com.databricks.sql.managedcatalog.command.GrantPermissionsCommandV2.run(PermissionCommands.scala:120)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:78)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:89)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:229)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:243)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:392)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:188)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:142)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:342)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:229)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:214)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:227)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:220)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:99)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:220)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:220)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:174)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:165)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:238)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:107)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:104)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:820)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:815)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n\u001b[0;32m<command-607812982174359>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0m_sqldf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m____databricks_percent_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0m____databricks_percent_sql\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m<command-607812982174359>\u001b[0m in \u001b[0;36m____databricks_percent_sql\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m____databricks_percent_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandard_b64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"R1JBTlQgU0VMRUNUIE9OIENBVEFMT0cgbWFpbiBUTyBtYWluZ3JvdXA=\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n\n\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o349.sql.\n: com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User is not an owner of Catalog 'main'.\n\tat com.databricks.managedcatalog.UCReliableHttpClient.reliablyAndTranslateExceptions(UCReliableHttpClient.scala:47)\n\tat com.databricks.managedcatalog.UCReliableHttpClient.patchJson(UCReliableHttpClient.scala:114)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$updatePermissions$1(ManagedCatalogClientImpl.scala:1806)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapException$1(ManagedCatalogClientImpl.scala:2913)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:25)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:23)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:78)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:2912)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.updatePermissions(ManagedCatalogClientImpl.scala:1801)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.addPermissions(ManagedCatalogCommon.scala:702)\n\tat com.databricks.sql.managedcatalog.command.GrantPermissionsCommandV2.run(PermissionCommands.scala:120)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:78)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:89)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:229)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:243)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:392)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:188)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:342)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:229)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:214)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:227)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:220)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:99)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:220)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:220)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:174)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:165)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:238)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:107)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:104)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:815)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User is not an owner of Catalog 'main'.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql GRANT SELECT ON CATALOG main TO maingroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5531401-835e-41a7-9235-9798f3ebb6fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-r-xr-xr-x 1 root root  4440017 May  3 00:31 ----ws_3_3--mvn--hadoop3--net.snowflake--snowflake-ingest-sdk--net.snowflake__snowflake-ingest-sdk__0.9.6.jar\n",
      "-r-xr-xr-x 1 root root      642 May  3 00:31 ----ws_3_3--vendor--snowflake--libsnowflake_resources.jar\n",
      "-r-xr-xr-x 1 root root   759953 May  3 00:31 ----ws_3_3--vendor--snowflake--snowflake-hive-2.3__hadoop-3.2_2.12_deploy.jar\n",
      "-rwxr-xr-x 1 root root 33419306 May  3 00:33 snowflake_jdbc_3_13_30.jar\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "ls -l /databricks/jars/ |  grep snow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f5daa6-80ff-4afc-8f63-0f98d7445903",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "rm /databricks/jars/*snowflake-jdbc* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66fc7838-ee8e-44a0-9ac7-3e18235dbc9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">res0: Boolean = true\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">res0: Boolean = true\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "mv  /FileStore/tables/snowflake_jdbc_3_13_30.jar /xin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48695d3a-4969-49ff-a3c9-df6e88906649",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">res2: Boolean = true\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">res2: Boolean = true\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs mv /xin /tmp/snowflake_jdbc_3_13_30.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad294b82-4a70-4c09-b6d7-ab7b4d4821b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat '/dbfs/xin/snowflake_jdbc_3_13_30.jar': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "cp /dbfs/xin/snowflake_jdbc_3_13_30.jar /databricks/jars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71093fa7-19a4-4e6d-8a42-8235a802cfd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 108 bytes.\n",
      "Out[5]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.put(\"/tmp/snowflake.sh\",\"\"\"\n",
    "#!/bin/bash\n",
    "rm /databricks/jars/*snowflake-jdbc* \n",
    "cp /dbfs/xin/snowflake_jdbc_3_13_30.jar /databricks/jars/\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31568736-8b27-4db6-be83-d8f93318193c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4272931785216578,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "UC",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
